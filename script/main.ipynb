{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "class CustomRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        random_state,\n",
    "        format_input=True,\n",
    "        format_output=True,\n",
    "        dt_params_dist_1={\n",
    "            'criterion': ['friedman_mse', 'absolute_error'],\n",
    "            'max_depth': Integer(2, 10),\n",
    "            'min_samples_split': Integer(2, 20),\n",
    "            'max_features': Categorical(['sqrt', 'log2', None]),\n",
    "        },\n",
    "        ada_params_dist={\n",
    "            'loss': ['linear', 'square'],\n",
    "            'n_estimators': Integer(10, 100),\n",
    "            'learning_rate': Real(1e-2, 1e0, prior='log-uniform'),\n",
    "        },\n",
    "        dt_params_dist_2={\n",
    "            'criterion': ['friedman_mse', 'absolute_error'],\n",
    "            'max_depth': Integer(2, 5), # 用于蒸馏ada2，因此应该浅一些\n",
    "            'min_samples_split': Integer(2, 20),\n",
    "            'max_features': Categorical(['sqrt', 'log2', None]), #\n",
    "        },\n",
    "        bayes_search_param={\n",
    "            'n_iter': 20,\n",
    "            'n_points': 5, # 使用比较大的n_initial_points参数可以显著减少\"The objective has been evaluated at this point before\"的警告\n",
    "            'cv': 5,\n",
    "            'scoring': 'neg_mean_squared_error',\n",
    "            'n_jobs': -1,\n",
    "            'verbose': 1\n",
    "        },\n",
    "        RFECV_param={\n",
    "        'step': 0.05,\n",
    "        'cv': 5,\n",
    "        'scoring': 'neg_mean_squared_error',\n",
    "        'min_features_to_select': 10\n",
    "        }\n",
    "    ):\n",
    "        self.random_state = random_state\n",
    "        self.format_input = format_input\n",
    "        self.format_output = format_output\n",
    "\n",
    "        # trian模式需要训练，从而需要定义用于训练的模型\n",
    "        # 定义各模型的超参数区间及搜索模型\n",
    "        self.dt_params_dist_1 = dt_params_dist_1\n",
    "        self.ada_params_dist=ada_params_dist\n",
    "        self.dt_params_dist_2 = dt_params_dist_2\n",
    "        self.bayes_search_param = bayes_search_param\n",
    "        self.RFECV_param = RFECV_param\n",
    "\n",
    "        # 定义模型\n",
    "        self.dt = DecisionTreeRegressor(random_state=self.random_state)\n",
    "        self.ada = AdaBoostRegressor(estimator=None, random_state=self.random_state)\n",
    "\n",
    "        self.bayes_search = BayesSearchCV(\n",
    "            estimator=self.dt, # 形式上的参数，fit()中会更改，相当于None\n",
    "            search_spaces=self.dt_params_dist_1, # 形式上的参数，fit()中会更改，相当于None\n",
    "            random_state=self.random_state\n",
    "        ).set_params(**self.bayes_search_param)\n",
    "\n",
    "        self.rfecv = RFECV(estimator=self.dt).set_params(**self.RFECV_param)  # 形式上的参数`estimator`，fit()中会更改，相当于None\n",
    "\n",
    "    def _format(self, X, mode):\n",
    "        # Convert to numpy array if DataFrame or Series\n",
    "        if isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "            X = X.to_numpy()\n",
    "\n",
    "        # For 2D arrays with shape (n_samples, 1), which are likely target variables (y)\n",
    "        # Convert to 1D array with shape (n_samples,)\n",
    "        if len(X.shape) == 2 and X.shape[1] == 1 and mode == 'y':\n",
    "            X = X.ravel()\n",
    "\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.format_input:\n",
    "            X = self._format(X, mode='X')\n",
    "            y = self._format(y, mode='y')\n",
    "\n",
    "        # 传递模型的pipeline\n",
    "        # self.model在fit前都需要转变为clone(self.model)防止__init__中的self.model被链接并拟合\n",
    "        # 寻找最优决策树\n",
    "        dt_1_bayes_search = clone(self.bayes_search).set_params(estimator=clone(self.dt), search_spaces=self.dt_params_dist_1)\n",
    "        # 将一个估计器（如决策树dt）传递给BayesSearchCV的estimator参数时，在BayesSearchCV拟合后，这个估计器也会被拟合，并被设置为性能最好的超参数组合下拟合得到的决策树\n",
    "        # 所以也需要对dt进行clone，防止__init__中的self.dt被链接并拟合\n",
    "        dt_1_bayes_search.fit(X, y)\n",
    "        dt_1_best = dt_1_bayes_search.best_estimator_\n",
    "\n",
    "        # 将最优决策树输入adaboost，寻找最优adaboost\n",
    "        ada_1 = clone(self.ada).set_params(estimator=dt_1_best)\n",
    "        ada_1_bayes_search = clone(self.bayes_search).set_params(estimator=ada_1, search_spaces=self.ada_params_dist)\n",
    "        ada_1_bayes_search.fit(X, y)\n",
    "        self.ada_best_ = ada_1_bayes_search.best_estimator_\n",
    "\n",
    "        if X.shape[1] >= 2:\n",
    "            rfecv = clone(self.rfecv).set_params(estimator=self.ada_best_)\n",
    "            rfecv.fit(X, y)\n",
    "            # Store the RFECV support mask\n",
    "            self.rfecv_support_ = rfecv.support_\n",
    "            X_selected = rfecv.transform(X)\n",
    "\n",
    "            # 将最优特征子集输入决策树，寻找最优决策树\n",
    "            dt_2_bayes_search = clone(self.bayes_search).set_params(estimator=clone(self.dt), search_spaces=self.dt_params_dist_2)\n",
    "            dt_2_bayes_search.fit(X_selected, y)\n",
    "            dt_2_best = dt_2_bayes_search.best_estimator_\n",
    "\n",
    "            # 将最优决策树输入adaboost，寻找最优adaboost\n",
    "            ada_2 = clone(self.ada).set_params(estimator=dt_2_best)\n",
    "            ada_2_bayes_search = clone(self.bayes_search).set_params(estimator=ada_2, search_spaces=self.ada_params_dist)\n",
    "            ada_2_bayes_search.fit(X_selected, y)\n",
    "            self.ada_best_ = ada_2_bayes_search.best_estimator_\n",
    "\n",
    "        return self # 为了链式调用，即CustomRegressor.fit(X, y).xxx == self.xxx\n",
    "\n",
    "    def predict(self, X, ada_best=None, rfecv_support_=None):\n",
    "        if self.format_input and isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "            self.index = X.index\n",
    "            X = self._format(X, mode='X')\n",
    "\n",
    "        if ada_best is not None:\n",
    "            self.ada_best_ = ada_best\n",
    "\n",
    "        if rfecv_support_ is not None:\n",
    "            self.rfecv_support_ = rfecv_support_\n",
    "\n",
    "        # Apply feature selection if it was used during training\n",
    "        if hasattr(self, 'rfecv_support_') and self.rfecv_support_ is not None:\n",
    "            # If X is a numpy array\n",
    "            X = X[:, self.rfecv_support_]\n",
    "\n",
    "        if self.format_output and hasattr(self, 'index'):\n",
    "            # Convert the predictions to a DataFrame with the original index\n",
    "            return pd.DataFrame(self.ada_best_.predict(X), index=self.index)\n",
    "        else:\n",
    "            # If the input is not a DataFrame, return the predictions as a numpy array\n",
    "            return self.ada_best_.predict(X)\n",
    "\n",
    "    # analog fit_transform from transformer\n",
    "    def fit_predict(self, X, y):\n",
    "        return self.fit(X, y).predict(X)\n",
    "\n",
    "class ComplementRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"A regressor that returns 1 minus the predictions of a base regressor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_regressor : estimator object\n",
    "        The base regressor whose predictions will be complemented.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_regressor=None):\n",
    "        self.base_regressor = base_regressor\n",
    "\n",
    "        # Clone the base_regressor to avoid modifying the original\n",
    "        self.base_regressor_ = clone(self.base_regressor)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the base regressor to the training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        self.base_regressor_.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y):\n",
    "        \"\"\"Return 1 minus the predictions of the base regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Test data.\n",
    "        y : array-like, shape (n_samples,)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y - self.base_regressor_.predict(X) : array-like, shape (n_samples,)\n",
    "            The complement of the predictions of the base regressor.\n",
    "        \"\"\"\n",
    "        # Return the complement\n",
    "        return y - self.base_regressor_.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import xgboost as xgb\n",
    "from tslearn.clustering import KShape\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from econml.dml import CausalForestDML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "import io\n",
    "from PIL import Image\n",
    "from factors_risks_dicts_generator import generate_factors_risks_dicts\n",
    "\n",
    "class Modeler():\n",
    "\n",
    "    def __init__(self, mode='train', factors_datas_names=[\n",
    "        'factors_data',\n",
    "        'fundamentals_data',\n",
    "        'macros_data',\n",
    "        'money_flows_data',\n",
    "        'securities_margins_data',\n",
    "        'industries_data',\n",
    "        'indexes_data'\n",
    "    ],\n",
    "    other_datas_names=[\n",
    "        'bank_stocks_info',\n",
    "        'returns_data',\n",
    "        'FCF_discounted_model_params_data'\n",
    "    ],\n",
    "    random_state=20250301\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.random_state = random_state\n",
    "        # 将所有factors_datas以外的数据都定义为类属性\n",
    "        for other_data_name in other_datas_names:\n",
    "            other_data = pickle.load(open(f'../data/exported_data/{other_data_name}_{mode}.pkl', mode='rb+'))\n",
    "            if other_data_name == 'bank_stocks_info':\n",
    "                self.industry_stocks_info = other_data\n",
    "            elif other_data_name == 'returns_data':\n",
    "                other_data['market_returns'].rename(columns={'000001.XSHG': 0}, inplace=True)\n",
    "                self.returns_data = other_data\n",
    "            elif other_data_name == 'FCF_discounted_model_params_data':\n",
    "                self.FCF_discounted_model_params_data = other_data\n",
    "\n",
    "        self.index = self.FCF_discounted_model_params_data['r_wacc'].index\n",
    "        self.columns = self.FCF_discounted_model_params_data['r_wacc'].columns\n",
    "\n",
    "        # 合并多个因子数据表\n",
    "        factors_datas = {}\n",
    "        # 导入数据\n",
    "        for factors_data_name in factors_datas_names:\n",
    "            factors_data = pickle.load(open(f'../data/exported_data/{factors_data_name}_{mode}.pkl', mode='rb+'))\n",
    "            factors_datas[factors_data_name] = factors_data\n",
    "\n",
    "        self.factors_datas = factors_datas\n",
    "        pickle.dump(obj=factors_datas, file=open(file=f'../data/exported_data/factors_datas_{self.mode}.pkl', mode='wb+'), protocol=4)\n",
    "\n",
    "    # 检查各因子是否为空表\n",
    "    def check_factor_data_nan(self):\n",
    "        return {\n",
    "            factors_data_name: [\n",
    "                factor_data_name\n",
    "                for factor_data_name, factor_data in factors_data.items()\n",
    "                if not factor_data.any().any()\n",
    "            ]\n",
    "            for factors_data_name, factors_data in tqdm(\n",
    "                self.factors_datas.items(),\n",
    "                desc='handling factors datas missing values progress'\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def _fill_nan_col(self, factor_data_without_type1_missing, factor_data):\n",
    "        #print('factor_data_without_type1_missing', factor_data_without_type1_missing)\n",
    "        # 找到factor_data_without_type1_missing中全为缺失值的列名\n",
    "        missing_stocks_codes = factor_data_without_type1_missing.columns[factor_data_without_type1_missing.isnull().all()]\n",
    "        print('missing_stocks_codes', missing_stocks_codes)\n",
    "        # 导出对应列在factor_data各行的分位数数据表，索引为缺失值列名\n",
    "        missing_stocks_codes_quantiles = factor_data.rank(pct=True, axis=1)[missing_stocks_codes]\n",
    "        # 计算对应列在factor_data各行的分位数据表的平均值series，索引为缺失值列名\n",
    "        missing_stocks_codes_quantiles_mean = missing_stocks_codes_quantiles.mean()\n",
    "        #print('missing_stocks_codes_quantiles_mean', missing_stocks_codes_quantiles_mean)\n",
    "        # fill_values是factor_data_without_type1_missing中全为缺失值的各列在对应分位数平均值处的值series，索引为列名\n",
    "        fill_values = factor_data_without_type1_missing.quantile(missing_stocks_codes_quantiles_mean, axis=1)\n",
    "\n",
    "        # 兼容fill_values与factor_data_without_type1_missing\n",
    "        fill_values = fill_values.T\n",
    "        fill_values.columns = missing_stocks_codes\n",
    "\n",
    "        # 用以上series填充对应缺失值列的缺失值\n",
    "        factor_data_without_type1_missing.update(fill_values) # 不可将df赋给df，前者这样应该取values，变为np.array，但是还是会warning，所以使用update\n",
    "        #print('fill_values', fill_values)\n",
    "        #print('factor_data_without_type1_missing_filled', factor_data_without_type1_missing)\n",
    "\n",
    "        return factor_data_without_type1_missing\n",
    "\n",
    "    # 缺失值处理\n",
    "    def _handle_missing_values(self, factor_data):\n",
    "        '''factor_data中有些因子值缺失，而这样缺失值要么是由于个股在上市前或者退市后，因子值不存在；要么是因为个股在市期间，其因子值没有被披露或被统计。在进行缺失值\n",
    "        处理时，忽略前一种缺失值，而填充后一种缺失值。\n",
    "            填充缺失值一般有三种方法，即SimpleImputation，KNNImputation和IterativeImputation。对于本面板数据，SimpleImputation（如均值填充或中位数填充）可能不\n",
    "        适合，因为它没有考虑时间序列的特性和个股之间的相关性。简单地用一个常数填充缺失值可能会引入偏差，尤其是当缺失值的比例较高时。而KNNImputation可以考虑个股之间的\n",
    "        相关性，但它也没有考虑时间序列的特性。此外，KNNImputation在处理大规模面板数据时可能会比较慢，因为它需要计算所有个股之间的距离矩阵。所以应该选择IterativeImpu\n",
    "        -tion。优缺点：考虑时间序列的特性，个股之间的相关性。\n",
    "            然而，在使用IterativeImputer填充所有缺失值后再删除第一类缺失值可能不是最佳方案。这是因为IterativeImputer在估计缺失值时会考虑所有的特征，包括那些本不应\n",
    "        该存在和被填充的第一类缺失值。这可能会影响估计的质量。\n",
    "            所以，以下方案是更好的选择。首先，识别出那些不包含第一类缺失值的样本日期，并仅使用这些样本来训练IterativeImputer。然后，使用训练后的IterativeImputer来\n",
    "        估计所有样本中的第二类缺失值。'''\n",
    "        # 定义一个掩码mask,标识每个个股在每个时间点上是否处于上市状态\n",
    "        mask = pd.DataFrame(index=self.index, columns=self.columns)\n",
    "        # 传出因子数据表每一行axis=1的中位数，组成各截面的中位数向量medians\n",
    "        medians = factor_data.median(axis=1)\n",
    "        # 对于每个代码为stock_code的个股\n",
    "        for stock_code in mask.columns:\n",
    "            # 对于代码为stock_code的个股，查找板块个股代码列表industry_stocks_info对应的个股代码，传出其上市日期start_date和退市日日期end_date\n",
    "            start_date = self.industry_stocks_info.loc[stock_code, 'start_date']\n",
    "            end_date = self.industry_stocks_info.loc[stock_code, 'end_date']\n",
    "            # 标识代码为stock的个股在每个时间点上是否处于上市状态\n",
    "            mask[stock_code] = (mask.index >= start_date) & (mask.index <= end_date)\n",
    "\n",
    "            # 对于代码为stock_code的个股，如果其在各时间上的因子值factor_data[stock_code]均为缺失值np.nan\n",
    "            if factor_data[stock_code].isnull().all():\n",
    "                #print('factor_data', factor_data, 'stock_code', stock_code)\n",
    "                # 将此代码为stock_code的个股其在各时间上的因子值factor_data[stock_code]传为各截面的中位数向量medians，防止被Imputer忽略\n",
    "                medians = pd.DataFrame(medians)\n",
    "                medians.columns = [stock_code]\n",
    "                factor_data.update(medians)\n",
    "                #print('factor_data', factor_data)\n",
    "\n",
    "        # 根据掩码mask，传出不包含第一类缺失值(不在市)的日期索引indexes_without_type1_missing\n",
    "        indexes_without_type1_missing = mask.all(axis=1)\n",
    "        # 定义不包含第一类缺失值的日期索引indexes_without_type1_missing对应的因子数据样本factor_data_without_type1_missing\n",
    "        factor_data_without_type1_missing = factor_data.loc[indexes_without_type1_missing]\n",
    "\n",
    "        # 对于factor_data_without_type1_missing中全为缺失值的列，得到其在整个数据factor_data中的分位数平均值，取此分位数在factor_data_without_type1_missing中各行对应的值填充缺失值列\n",
    "        factor_data_without_type1_missing = self._fill_nan_col(factor_data_without_type1_missing, factor_data)\n",
    "\n",
    "        # 定义IterativeImputer，所有缺失值被填充后需要再次加入训练，再次填充原有缺失值，直至缺失值收敛，这样的递归次数max_iter为50，随机种子random_state为self.random_state\n",
    "        imputer = IterativeImputer(\n",
    "            random_state=self.random_state,\n",
    "            # 使用XGBoost填充缺失值\n",
    "            estimator=xgb.XGBRegressor(),\n",
    "            max_iter=50,\n",
    "            tol=1e-3\n",
    "            )\n",
    "\n",
    "        # 使用factor_data_without_type1_missing来训练IterativeImputer\n",
    "        imputer.fit(factor_data_without_type1_missing)\n",
    "        # 利用imputer填充factor_data的全部缺失值，传出为填充后因子数据表factor_data_imputed\n",
    "        factor_data_imputed = imputer.transform(factor_data)\n",
    "        factor_data_imputed = pd.DataFrame(factor_data_imputed)\n",
    "        factor_data_imputed.index = self.index\n",
    "        factor_data_imputed.columns = self.columns\n",
    "\n",
    "        # 将填充后因子数据表factor_data_imputed中的第一类缺失值重新标记为np.nan，即使用训练后的imputer来估计所有因子数据表中的第二类缺失值\n",
    "        factor_data_imputed[~mask] = np.nan\n",
    "\n",
    "        return factor_data_imputed\n",
    "\n",
    "    # Fama-French-3分位数差值处理\n",
    "    def _process_ff3_quantile_difference(self, factor_data):\n",
    "        '''FF3处理形成截面股价收益率:\n",
    "        合理性:如果您的研究目的是探究因子对股价收益率的截面预测能力,并且假设股价收益率的截面分布与因子的截面分布相关,那么按照FF3处理形成截面股价收益率是合适的。\n",
    "        优点:这种方法能够消除股价收益率的极值影响,使得截面股价收益率的分布更加稳定,便于研究因子的预测能力。\n",
    "        缺点:这种方法忽略了个股市值的影响,可能无法反映市场整体的收益率变化。'''\n",
    "        panal_factor_data_quantiles = factor_data.quantile([0.3, 0.7], axis=1)\n",
    "\n",
    "        return panal_factor_data_quantiles.loc[0.7] - panal_factor_data_quantiles.loc[0.3]\n",
    "\n",
    "    # 企业价值加权处理\n",
    "    def _average_by_enterprise_value(self, factor_data):\n",
    "        '''按个股市值加权形成截面股价收益率:\n",
    "        合理性:如果您的研究目的是探究因子对市场整体收益率的预测能力,并且假设个股的市值反映了其在市场中的重要性,那么按个股市值加权形成截面股价收益率是合适的。\n",
    "        优点:这种方法考虑了个股市值的影响,能够反映市场整体的收益率变化,更接近实际的投资组合收益。\n",
    "        缺点:这种方法可能受到大市值股票的主导,小市值股票的影响可能被掩盖。\n",
    "        就本文的银行板块研究目的来说，选择按个股市值加权形成截面股价收益率。'''\n",
    "        weighted_factor_data = self.enterprise_value_weights * factor_data\n",
    "        panal_factor_data = weighted_factor_data.sum(axis=1)\n",
    "\n",
    "        panal_factor_data = pd.DataFrame(panal_factor_data)\n",
    "        panal_factor_data.index = self.index\n",
    "        #panal_factor_data.columns = [factor_data_name] 不重置列名，防止与其他panal_factor_data运算时因为列名不一致而出现两行缺失值\n",
    "\n",
    "        return panal_factor_data\n",
    "\n",
    "    # 数据清理，即缺失值处理和Fama-French-3分位数差值处理\n",
    "    def clean_and_average_factors_datas(self):\n",
    "        self.enterprise_value_weights = self.FCF_discounted_model_params_data['panal_enterprise_value_weights']\n",
    "        industry_factors_datas = self.factors_datas.copy()\n",
    "\n",
    "        for factors_data_name, factors_data in self.factors_datas.items():\n",
    "            for factor_data_name, factor_data in tqdm(factors_data.items(), desc='handling factors data progress'):\n",
    "                if factors_data_name != 'macros_data': #factor_data_name == 'PEG':\n",
    "                    # 缺失值处理\n",
    "                    try:\n",
    "                        factor_data_imputed = self._handle_missing_values(factor_data)\n",
    "                    except:\n",
    "                        print(factor_data_name, factors_data_name)\n",
    "                        raise\n",
    "\n",
    "                    # 企业价值加权处理\n",
    "                    industry_factor_data = self._average_by_enterprise_value(factor_data_imputed)\n",
    "                    # 对r_wacc以enterprise_value在截面的权重加权求和求出enterprise_value加权r_wacc，\n",
    "                    #分别求出circulating_value加权r_E和Debts加权r_D，再以截面总circulating_value和总Debts在截面的总enterprise_value的权重加权得到enterprise_value加权r_wacc\n",
    "                    # 二者过程等价\n",
    "                    industry_factors_datas[factors_data_name][factor_data_name] = industry_factor_data\n",
    "\n",
    "        self.industry_factors_datas = industry_factors_datas\n",
    "\n",
    "        return industry_factors_datas\n",
    "\n",
    "    def clean_and_average_r_wacc_data(self):\n",
    "        self.enterprise_value_weights = self.FCF_discounted_model_params_data['panal_enterprise_value_weights']\n",
    "        r_wacc = self.FCF_discounted_model_params_data['r_wacc']\n",
    "\n",
    "        r_wacc_imputed = self._handle_missing_values(r_wacc)\n",
    "\n",
    "        industry_r_wacc = self._average_by_enterprise_value(r_wacc_imputed)\n",
    "\n",
    "        industry_r_wacc_data = {'r_wacc': industry_r_wacc}\n",
    "\n",
    "        self.industry_r_wacc_data = industry_r_wacc_data\n",
    "\n",
    "        return industry_r_wacc_data\n",
    "\n",
    "    def transform_factors_datas_from_dict_to_df(self):\n",
    "        \"\"\"Transform dictionary of factors data into a single DataFrame more efficiently using concat.\"\"\"\n",
    "        # Create a list to hold all individual factor DataFrames\n",
    "        factor_dfs = []\n",
    "\n",
    "        # Iterate through all industry_factors_data dictionaries\n",
    "        for _, factors_data in self.industry_factors_datas.items():\n",
    "            # For each factor in the industry_factors_data dictionary\n",
    "            for factor_data_name, factor_data in factors_data.items():\n",
    "                # Create a copy of the DataFrame with the factor name as column name\n",
    "                factor_df = factor_data.copy()\n",
    "                # To prevent column name conflicts when factors have the same name across different data types,\n",
    "                # use a unique identifier by combining factor name with data source\n",
    "                factor_df.columns = [factor_data_name]\n",
    "                factor_dfs.append(factor_df)\n",
    "\n",
    "        # Concatenate all factor DataFrames at once (horizontally)\n",
    "        factors_df = pd.concat(factor_dfs, axis=1)\n",
    "        \n",
    "        # Create an IterativeImputer to fill any remaining missing values\n",
    "        imputer = IterativeImputer(\n",
    "            random_state=self.random_state,\n",
    "            # Use XGBoost for imputation\n",
    "            estimator=xgb.XGBRegressor(),\n",
    "            max_iter=50,\n",
    "            tol=1e-3\n",
    "        )\n",
    "    \n",
    "        # Apply the IterativeImputer to fill missing values\n",
    "        industry_factors_df = imputer.fit_transform(factors_df)\n",
    "        # Convert back to DataFrame with original index and column names\n",
    "        industry_factors_df = pd.DataFrame(industry_factors_df, index=self.index, columns=factors_df.columns)\n",
    "    \n",
    "        # Store the result as an instance variable\n",
    "        self.industry_factors_df = industry_factors_df\n",
    "        return industry_factors_df\n",
    "\n",
    "    # 定义标准化因子风险归类数据表\n",
    "    def standardize_factors_risks_data(self):\n",
    "        # 导入手动划分的因子风险归类数据表factors_risks_data，其有三列，分别是因子中文名称、因子代码和因子所属风险\n",
    "        try:\n",
    "            factors_risks_data = pd.read_csv('../data/dict/factors_risks_dicts.csv')\n",
    "        except FileNotFoundError:\n",
    "            generate_factors_risks_dicts()\n",
    "\n",
    "        # 定义因子代码列表\n",
    "        factors_codes = self.industry_factors_df.columns.tolist()\n",
    "\n",
    "        # 只保留最后一个重复值\n",
    "        factors_risks_data.drop_duplicates(subset=['factor_code'], keep='last', inplace=True)\n",
    "        factors_risks_data = factors_risks_data.set_index('factor_code')\n",
    "        # 将factors_risks_data中的risk列标准化为0-1变量\n",
    "        factors_risks_data_standardized = pd.DataFrame(0, columns=['default_risk', 'liquidity_risk', 'market_risk'], index=factors_risks_data.index)\n",
    "\n",
    "        # 使用向量化操作直接设置对应的风险类型为1\n",
    "        risk_series = factors_risks_data['risk']\n",
    "        factors_risks_data_standardized['default_risk'] = risk_series.str.contains('Default Risk', regex=False).astype(int)\n",
    "        factors_risks_data_standardized['liquidity_risk'] = risk_series.str.contains('Liquidity Risk', regex=False).astype(int)\n",
    "        factors_risks_data_standardized['market_risk'] = risk_series.str.contains('Market Risk', regex=False).astype(int)\n",
    "\n",
    "        # 检查是否有缺失的因子代码\n",
    "        missing_factors = [code for code in factors_codes if code not in factors_risks_data_standardized.index]\n",
    "        \n",
    "        # 为缺失的因子代码添加新行，用随机的0或1填充风险列\n",
    "        if missing_factors:\n",
    "            # 设置随机种子以保持可重复性\n",
    "            random.seed(self.random_state)\n",
    "            \n",
    "            # 为每个缺失的因子创建新的行\n",
    "            missing_data = pd.DataFrame(\n",
    "            index=missing_factors,\n",
    "            columns=['default_risk', 'liquidity_risk', 'market_risk'],\n",
    "            data=[[random.randint(0, 1) for _ in range(3)] for _ in range(len(missing_factors))]\n",
    "            )\n",
    "            \n",
    "            # 添加到标准化的数据框中\n",
    "            factors_risks_data_standardized = pd.concat([factors_risks_data_standardized, missing_data])\n",
    "        \n",
    "        # 删除factors_risks_data_standardized中不在factors_codes中的行\n",
    "        factors_risks_data_standardized = factors_risks_data_standardized.loc[factors_risks_data_standardized.index.isin(factors_codes)]\n",
    "\n",
    "        # 使用KShape对factor时间序列进行聚类\n",
    "        # 提取factor时间序列\n",
    "        factors_ts = self.industry_factors_df.copy()\n",
    "\n",
    "        # 标准化时间序列数据用于聚类\n",
    "        scaler = StandardScaler()\n",
    "        factors_ts_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(factors_ts),\n",
    "            index=factors_ts.index,\n",
    "            columns=factors_ts.columns\n",
    "        )\n",
    "\n",
    "        # 定义风险类型\n",
    "        init_centroids = []\n",
    "        risk_types = ['default_risk', 'liquidity_risk', 'market_risk']\n",
    "\n",
    "        for risk_type in risk_types:\n",
    "            # 获取属于该风险类型的因子名称列表\n",
    "            risk_factors_codes = factors_risks_data_standardized.loc[\n",
    "                factors_risks_data_standardized[risk_type] == 1\n",
    "            ].index.tolist()\n",
    "\n",
    "            # 从标准化后的时间序列数据中提取这些因子的数据并计算平均值\n",
    "            risk_centroid = factors_ts_scaled[risk_factors_codes].mean(axis=1).values\n",
    "            init_centroids.append(risk_centroid)\n",
    "\n",
    "        # 转换为正确的形状以适配KShape要求\n",
    "        init_centroids = np.array(init_centroids)\n",
    "        init_centroids = init_centroids.reshape(len(risk_types), len(factors_ts_scaled), 1)\n",
    "        # 这里的`init_centroids`是一个3D数组，表示3个初始质心，每个质心对应一个聚类中心\n",
    "\n",
    "        # 执行KShape聚类\n",
    "        n_clusters = 3  # 假设我们希望聚类为3种风险类型\n",
    "        kshape = KShape(n_clusters=n_clusters, random_state=self.random_state, init=init_centroids)\n",
    "\n",
    "        # 创建适合KShape的输入数据格式\n",
    "        factors_ts_tensor = np.zeros((len(factors_ts_scaled.columns), len(factors_ts_scaled), 1))\n",
    "        factors_ts_tensor = factors_ts_scaled.values.T.reshape(len(factors_ts_scaled.columns), len(factors_ts_scaled), 1)\n",
    "\n",
    "        kshape.fit(factors_ts_tensor)\n",
    "\n",
    "        # 获取聚类标签\n",
    "        cluster_labels = kshape.labels_\n",
    "\n",
    "        # 将聚类标签重构为新的factors_risks_data_standardized\n",
    "        # Map cluster IDs to column indices\n",
    "        for i, risk_type in enumerate(risk_types):\n",
    "            # For each risk type, set 1 for factors that belong to that cluster\n",
    "            factors_risks_data_standardized[risk_type] = (cluster_labels == i).astype(int)\n",
    "\n",
    "        self.industry_factors_df = factors_ts_scaled\n",
    "        self.factors_risks_data_standardized = factors_risks_data_standardized\n",
    "\n",
    "        return factors_risks_data_standardized\n",
    "\n",
    "    def visualize_risk_factors_clusters(\n",
    "        self,\n",
    "        windows=(1, 30, 120),\n",
    "        risk_types=None,\n",
    "        num_factors=3,\n",
    "        colors=None,\n",
    "        linestyles=None,\n",
    "        figsize=None,\n",
    "        random_seed=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Visualize moving averages of risk factors grouped by risk type.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        windows : tuple\n",
    "            Window sizes for moving averages\n",
    "        risk_types : list, optional\n",
    "            List of risk types to include. If None, uses ['default_risk', 'liquidity_risk', 'market_risk']\n",
    "        num_factors : int\n",
    "            Number of factors to randomly sample for each risk type\n",
    "        colors : list, optional\n",
    "            List of colors for plotting different factors. If None, uses ['blue', 'green', 'red']\n",
    "        linestyles : tuple, optional\n",
    "            Tuple of linestyles for different window sizes. If None, uses (':', '--', '-')\n",
    "        figsize : tuple, optional\n",
    "            Figure size (width, height) in inches. If None, calculates based on number of rows and columns\n",
    "        random_seed : int, optional\n",
    "            Random seed for reproducibility when sampling factors. If None, uses self.random_state\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            The figure containing the visualized moving averages\n",
    "        \"\"\"\n",
    "        # Set default values\n",
    "        if risk_types is None:\n",
    "            risk_types = ['default_risk', 'liquidity_risk', 'market_risk']\n",
    "        if colors is None:\n",
    "            colors = ['blue', 'green', 'red']\n",
    "        if linestyles is None:\n",
    "            linestyles = (':', '--', '-')\n",
    "        if random_seed is None:\n",
    "            random_seed = self.random_state\n",
    "            \n",
    "        # Set random seed for reproducibility\n",
    "        random.seed(random_seed)\n",
    "        \n",
    "        # Group factors by risk type\n",
    "        sampled_risks_factors_codes = {}\n",
    "        \n",
    "        # For each risk type, randomly select factors\n",
    "        for risk_type in risk_types:\n",
    "            # Get factors that belong to this risk type (where value is 1)\n",
    "            risk_factors_codes = self.factors_risks_data_standardized[\n",
    "                self.factors_risks_data_standardized[risk_type] == 1\n",
    "            ].index.tolist()\n",
    "            \n",
    "            # Randomly sample factors (or fewer if not enough)\n",
    "            sampled_risks_factors_codes[risk_type] = random.sample(\n",
    "                risk_factors_codes,\n",
    "                k=min(num_factors, len(risk_factors_codes))\n",
    "            )\n",
    "        \n",
    "        # Flatten the dictionary to get all sampled factors\n",
    "        sampled_factors_codes = [\n",
    "            sampled_risk_factor_code\n",
    "            for sampled_risk_factors_codes in sampled_risks_factors_codes.values()\n",
    "            for sampled_risk_factor_code in sampled_risk_factors_codes\n",
    "        ]\n",
    "        \n",
    "        # Create displayed dataframe with these factors\n",
    "        sampled_factors_ts = self.industry_factors_df[sampled_factors_codes]\n",
    "        \n",
    "        # Setup subplot grid dimensions\n",
    "        num_row = len(windows)\n",
    "        num_cols = len(risk_types)\n",
    "        \n",
    "        # Calculate figure size if not provided\n",
    "        if figsize is None:\n",
    "            figsize = (5 * num_cols, 5 * num_row)\n",
    "        \n",
    "        # Create figure and axes\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows=num_row,\n",
    "            ncols=num_cols,\n",
    "            figsize=figsize,\n",
    "            squeeze=False\n",
    "        )\n",
    "        \n",
    "        # First loop through windows (rows)\n",
    "        for i, window in enumerate(windows):\n",
    "            linestyle = linestyles[i] if i < len(linestyles) else '-'\n",
    "            # Then loop through risk types (columns)\n",
    "            for j, risk_type in enumerate(risk_types):\n",
    "                # Get the factors for this risk type\n",
    "                sampled_risk_factors_codes = sampled_risks_factors_codes[risk_type]\n",
    "\n",
    "                # Plot each factor in this risk type\n",
    "                for k, sampled_risk_factor_code in enumerate(sampled_risk_factors_codes):\n",
    "                    # Calculate moving average\n",
    "                    sampled_risk_factors_ts = sampled_factors_ts[sampled_risk_factor_code].rolling(window=window).mean()\n",
    "                    # Plot with different colors for different factors\n",
    "                    axes[i, j].plot(\n",
    "                        self.index,\n",
    "                        sampled_risk_factors_ts,\n",
    "                        label=sampled_risk_factor_code,\n",
    "                        color=colors[k % len(colors)],\n",
    "                        linestyle=linestyle\n",
    "                    )\n",
    "\n",
    "                # Set labels and title\n",
    "                axes[i, j].set_title(f'{window}-Day MA for {risk_type}')\n",
    "                axes[i, j].set_xlabel('Date')\n",
    "                axes[i, j].set_ylabel('Value')\n",
    "                axes[i, j].grid(True)\n",
    "                axes[i, j].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        return fig\n",
    "\n",
    "    # 取得系统性风险和非系统性风险风险溢价\n",
    "    def get_industry_risks_premiums_totals(self, risks_premiums_totals_curs=None):\n",
    "        self.industry_risks_premiums_totals = {}\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            assert risks_premiums_totals_curs is None, 'train模式下，风险溢价模型的初始值为None'\n",
    "            self.risks_premiums_totals_curs = {}\n",
    "        elif self.mode == 'test':\n",
    "            assert risks_premiums_totals_curs is not None, 'test模式下，风险溢价模型的初始值不能为None'\n",
    "            self.risks_premiums_totals_curs = risks_premiums_totals_curs\n",
    "\n",
    "        # 载入数据\n",
    "        r_wacc = self.industry_r_wacc_data['r_wacc']\n",
    "        interest_rate_1m = self.FCF_discounted_model_params_data['interest_rate_1m']\n",
    "        market_returns = self.returns_data['market_returns']\n",
    "\n",
    "        # 定义r_wacc风险溢价\n",
    "        self.industry_risk_premium = r_wacc - interest_rate_1m\n",
    "        market_risk_premium_common = market_returns - interest_rate_1m\n",
    "\n",
    "        cur = CustomRegressor(random_state=self.random_state)\n",
    "        if self.mode == 'train':\n",
    "            industry_risk_premium_common = cur.fit_predict(market_risk_premium_common, self.industry_risk_premium)\n",
    "            self.risks_premiums_totals_curs['common_risk'] = cur\n",
    "\n",
    "            # 创建一个新的CustomRegressor对象用于idiosyncratic_risk\n",
    "            idio_cur = CustomRegressor(random_state=self.random_state)\n",
    "            idio_cur.ada_best_ = ComplementRegressor(\n",
    "            base_regressor=cur.ada_best_\n",
    "            )\n",
    "            self.risks_premiums_totals_curs['idiosyncratic_risk'] = idio_cur\n",
    "\n",
    "        elif self.mode == 'test':\n",
    "            industry_risk_premium_common = cur.predict(\n",
    "                market_risk_premium_common,\n",
    "                self.risks_premiums_totals_curs['common_risk'].ada_best_,\n",
    "                self.risks_premiums_totals_curs['common_risk'].rfecv_support_\n",
    "            )\n",
    "\n",
    "        self.industry_risks_premiums_totals['common_risk'] = industry_risk_premium_common\n",
    "        industry_risk_premium_idiosyncratic = self.industry_risk_premium - industry_risk_premium_common\n",
    "        self.industry_risks_premiums_totals['idiosyncratic_risk'] = industry_risk_premium_idiosyncratic\n",
    "\n",
    "        return self.industry_risks_premiums_totals, self.risks_premiums_totals_curs\n",
    "\n",
    "    def get_industry_risks_premiums_components(self, risks_premiums_components_curs=None):\n",
    "        self.industry_risks_premiums_components = {}\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            assert risks_premiums_components_curs is None, 'train模式下，风险溢价模型的初始值为None'\n",
    "            self.risks_premiums_components_curs = {}\n",
    "        elif self.mode == 'test':\n",
    "            assert risks_premiums_components_curs is not None, 'test模式下，风险溢价模型的初始值不能为None'\n",
    "            self.risks_premiums_components_curs = risks_premiums_components_curs\n",
    "\n",
    "        for risk_premium_component_name in {'default_risk', 'liquidity_risk','market_risk'}:\n",
    "            risk_premium_component_factors_codes = self.factors_risks_data_standardized.loc[self.factors_risks_data_standardized[risk_premium_component_name] == 1].index.tolist()\n",
    "            risk_premium_component_factors_df = self.industry_factors_df[risk_premium_component_factors_codes]\n",
    "\n",
    "            cur = CustomRegressor(random_state=self.random_state)\n",
    "            if self.mode == 'train':\n",
    "                risk_premium_component = cur.fit_predict(risk_premium_component_factors_df, self.industry_risk_premium)\n",
    "                self.risks_premiums_components_curs[risk_premium_component_name] = cur\n",
    "\n",
    "            elif self.mode == 'test':\n",
    "                risk_premium_component = cur.predict(\n",
    "                    risk_premium_component_factors_df,\n",
    "                    self.risks_premiums_components_curs[risk_premium_component_name]\n",
    "                    .ada_best_,\n",
    "                    self.risks_premiums_components_curs[risk_premium_component_name].rfecv_support_\n",
    "                )\n",
    "\n",
    "            self.industry_risks_premiums_components[risk_premium_component_name] = risk_premium_component\n",
    "\n",
    "        return self.industry_risks_premiums_components, self.risks_premiums_components_curs\n",
    "\n",
    "    def estimate_causal_effects_with_dml(self):\n",
    "        \"\"\"\n",
    "        使用DynamicDML估计三种风险溢价与总风险溢价之间的因果关系\n",
    "\n",
    "        针对每个风险溢价组件(default_risk, liquidity_risk, market_risk)作为处理变量，\n",
    "        其他两个风险溢价组件作为混淆变量，估计其对系统性和非系统性风险溢价的因果效应\n",
    "\n",
    "        Returns:\n",
    "            dict: 嵌套字典，格式为{处理变量: {结果变量: DynamicDML模型}}\n",
    "        \"\"\"\n",
    "\n",
    "        # 初始化结果字典\n",
    "        causal_forest_forests = {}\n",
    "\n",
    "        # 风险溢价组件\n",
    "        risk_components = ['default_risk', 'liquidity_risk', 'market_risk']\n",
    "        # 总风险溢价\n",
    "        risk_totals = ['common_risk', 'idiosyncratic_risk']\n",
    "\n",
    "        # 创建控制变量\n",
    "        index = pd.to_datetime(self.index)\n",
    "\n",
    "        # 第1个控制变量就是从1到len(self.index)的整数序列，表示时间序列的趋势性\n",
    "        X_1 = pd.Series(range(1, len(index) + 1), index=index, name='control_variable')\n",
    "\n",
    "        # 第2个到第12个控制变量表示self.index是否在某月份内，表示时间序列的季节性\n",
    "        X_2_to_12 = pd.get_dummies(index.month, prefix='month', drop_first=True)\n",
    "        X_2_to_12.index = index\n",
    "\n",
    "        # 将第1个控制变量与第2个到第12个控制变量合并为一个DataFrame\n",
    "        X = pd.concat([X_1, X_2_to_12], axis=1)\n",
    "        X.index = self.index\n",
    "\n",
    "        # 遍历每一个风险溢价作为处理变量\n",
    "        for treatment in risk_components:\n",
    "            causal_forest_forests[treatment] = {}\n",
    "\n",
    "            # 选择其他两个风险溢价作为混淆变量\n",
    "            confounders = [\n",
    "                confounder for confounder in risk_components\n",
    "                if confounder != treatment\n",
    "            ]\n",
    "\n",
    "            # 准备处理变量数据\n",
    "            T = self.industry_risks_premiums_components[treatment].to_numpy().ravel()\n",
    "\n",
    "            # 准备混淆变量数据 - 合并为一个DataFrame\n",
    "            W = pd.concat(\n",
    "                [\n",
    "                    self.industry_risks_premiums_components[confounder] for confounder in confounders\n",
    "                ],\n",
    "                axis=1\n",
    "            )\n",
    "            W.columns = confounders\n",
    "\n",
    "            # 遍历每一个总风险溢价作为结果变量\n",
    "            for outcome in risk_totals:\n",
    "                # 准备结果变量数据\n",
    "                Y = self.industry_risks_premiums_totals[outcome].to_numpy().ravel()\n",
    "\n",
    "                # 创建DynamicDML模型\n",
    "                cfdml = CausalForestDML(\n",
    "                    model_y=CustomRegressor(random_state=self.random_state, format_input=False),\n",
    "                    model_t=CustomRegressor(random_state=self.random_state, format_input=False),\n",
    "                    cv=5,  # 交叉验证折数\n",
    "                    n_estimators=200,  # 树的数量\n",
    "                    max_depth=6,  # 最大树深度\n",
    "                    min_samples_leaf=10,  # 叶节点最小样本数\n",
    "                    max_features='sqrt',  # 特征抽样策略\n",
    "                    honest=True,  # 使用诚实树提高推断可靠性\n",
    "                    random_state=self.random_state\n",
    "                )\n",
    "\n",
    "                # 拟合模型\n",
    "                cfdml.fit(Y, T, X=X, W=W)\n",
    "\n",
    "                # 存储模型\n",
    "                causal_forest_forests[treatment][outcome] = cfdml\n",
    "\n",
    "        # 将结果存储为类属性以便后续分析\n",
    "        self.causal_forest_forests = causal_forest_forests\n",
    "\n",
    "        return causal_forest_forests\n",
    "\n",
    "    def _construct_causal_data(self, treatment, outcome):\n",
    "        \"\"\"Generate enhanced visualizations for the specified causal model.\"\"\"\n",
    "        # Get the model\n",
    "        model = self.causal_forest_forests[treatment][outcome]\n",
    "        \n",
    "        # Prepare the data\n",
    "        index = pd.to_datetime(self.index)\n",
    "        X_1 = pd.Series(range(1, len(index) + 1), index=index, name='days_sequence')\n",
    "        X_2_to_12 = pd.get_dummies(index.month, prefix='month', drop_first=True)\n",
    "        X_2_to_12.index = index\n",
    "        X = pd.concat([X_1, X_2_to_12], axis=1)\n",
    "        X.index = self.index\n",
    "        \n",
    "        # Choose other risk components as confounders\n",
    "        risk_components = ['default_risk', 'liquidity_risk', 'market_risk']\n",
    "        confounders = [c for c in risk_components if c != treatment]\n",
    "        W = pd.concat([self.industry_risks_premiums_components[c] for c in confounders], axis=1)\n",
    "        W.columns = confounders\n",
    "\n",
    "        # Treatment and outcome variables\n",
    "        T = self.industry_risks_premiums_components[treatment].to_numpy().ravel()\n",
    "        Y = self.industry_risks_premiums_totals[outcome].to_numpy().ravel()\n",
    "\n",
    "        # Generate enhanced visualizations\n",
    "        return model, X, W, T, Y\n",
    "\n",
    "    def visualize_causal_forest_trees(self):\n",
    "        \"\"\"\n",
    "        Visualize the last decision tree for each combination of risk component and risk outcome.\n",
    "\n",
    "        Returns:\n",
    "            matplotlib.figure.Figure: The figure containing the visualized trees\n",
    "        \"\"\"\n",
    "        # Define risk types and outcomes to visualize\n",
    "        risk_components = ['default_risk', 'liquidity_risk', 'market_risk']\n",
    "        risk_totals = ['common_risk', 'idiosyncratic_risk']\n",
    "\n",
    "        # Create a figure with more vertical space for titles\n",
    "        fig, axes = plt.subplots(\n",
    "            len(risk_totals),\n",
    "            len(risk_components),\n",
    "            figsize=(30, 23),\n",
    "            dpi=300\n",
    "        )\n",
    "\n",
    "        # Then proceed with the individual tree visualizations\n",
    "        for row_idx, outcome in enumerate(risk_totals):\n",
    "            for col_idx, treatment in enumerate(risk_components):\n",
    "                # Get the current axis\n",
    "                ax = axes[row_idx, col_idx]\n",
    "\n",
    "                # Get the CausalForestDML model\n",
    "                cfdml_model = self.causal_forest_forests[treatment][outcome]\n",
    "\n",
    "                # Access the forest estimator from the CATE model\n",
    "                # Note: Accessing internal attributes like model_cate and estimators_ can be fragile\n",
    "                forest = cfdml_model.model_cate.estimators_[0]\n",
    "\n",
    "                # Get all trees in the forest\n",
    "                estimators = forest.estimators_\n",
    "                n_estimators = len(estimators)\n",
    "\n",
    "                # Get only the last tree\n",
    "                tree = estimators[-1]\n",
    "\n",
    "                # Create temp file for DOT data\n",
    "                dot_data = io.StringIO()\n",
    "\n",
    "                # Define feature names based on the structure used in _construct_causal_data\n",
    "                feature_names = ['days_sequence'] + [f'month_{m}' for m in range(2, 13)]\n",
    "\n",
    "                # Export tree to DOT format\n",
    "                export_graphviz(\n",
    "                    tree,\n",
    "                    out_file=dot_data,\n",
    "                    feature_names=feature_names, # Use defined feature names\n",
    "                    filled=True,\n",
    "                    rounded=True,\n",
    "                    special_characters=True,\n",
    "                    proportion=True # Show proportions instead of counts\n",
    "                )\n",
    "\n",
    "                # Create graphviz object\n",
    "                graph = graphviz.Source(dot_data.getvalue())\n",
    "\n",
    "                # Render to PNG\n",
    "                png_data = graph.pipe(format='png')\n",
    "\n",
    "                # Convert to image\n",
    "                img = Image.open(io.BytesIO(png_data))\n",
    "\n",
    "                # Display image\n",
    "                ax.imshow(np.array(img))\n",
    "\n",
    "                # Set subplot titles with smaller fontsize and better positioning\n",
    "                title_text = f\"{treatment} to {outcome}\\nTree #{n_estimators}/{n_estimators}\"\n",
    "                ax.set_title(title_text, fontsize=10, pad=2) # Adjusted fontsize and padding\n",
    "\n",
    "                # Remove ticks and labels but keep the frame (border) visible\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                # Ensure the frame/border is visible\n",
    "                ax.spines['top'].set_visible(True)\n",
    "                ax.spines['right'].set_visible(True)\n",
    "                ax.spines['bottom'].set_visible(True)\n",
    "                ax.spines['left'].set_visible(True)\n",
    "\n",
    "        # Add overall title with more space above the plot\n",
    "        plt.suptitle(\n",
    "            \"$\\mathsf{CausalForest}$'s Last $\\mathsf{DecisionTree}$ for Effect $\\operatorname{CATE}(z)$ of Each Risk Component on Each Risk Outcome\",\n",
    "            fontsize=20\n",
    "        )\n",
    "\n",
    "        # Adjust layout to make room for titles\n",
    "        plt.tight_layout()\n",
    "        #plt.subplots_adjust(top=1)  # Adjust top margin to make space for suptitle\n",
    "\n",
    "        # Return the figure without showing it\n",
    "        return fig\n",
    "\n",
    "    def visualize_causal_effect(self, treatment_name, outcome_name):\n",
    "        \"\"\"\n",
    "        Generate comprehensive visualizations for causal effects analysis.\n",
    "\n",
    "        Parameters:\n",
    "            treatment_name (str): Name of the treatment variable\n",
    "            outcome_name (str): Name of the outcome variable\n",
    "\n",
    "        Returns:\n",
    "            list: A list of matplotlib figure objects for the generated visualizations\n",
    "        \"\"\"\n",
    "        # Get effects and their confidence intervals\n",
    "        model, X, _, T, Y = self._construct_causal_data(treatment_name, outcome_name)\n",
    "\n",
    "        # Initialize list to store figures\n",
    "        figures = []\n",
    "\n",
    "        effects = model.effect(X)\n",
    "        effect_inference = model.effect_inference(X)\n",
    "        effects_intervals = effect_inference.conf_int()\n",
    "        # Get ATE as a scalar value\n",
    "        ate = model.ate(X)\n",
    "\n",
    "        # Get p-value for ATE - properly handle whether it's a method or attribute\n",
    "        ate_inference = model.ate_inference(X)\n",
    "        p_value = ate_inference.pvalue()\n",
    "\n",
    "        # ---------- 1. CATE Over Time ----------\n",
    "        fig1, ax1 = plt.subplots(figsize=(12, 6))\n",
    "        # Sort by time\n",
    "        time_values = X['days_sequence'].values\n",
    "        sort_idx = np.argsort(time_values)\n",
    "        sorted_time = time_values[sort_idx]\n",
    "        sorted_effects = effects[sort_idx]\n",
    "        # Plot CATE line and individual points (outliers/scatter)\n",
    "        ax1.plot(\n",
    "            sorted_time,\n",
    "            sorted_effects,\n",
    "            linewidth=1.5,\n",
    "            label='$\\operatorname{CATE}$'\n",
    "        )\n",
    "        ax1.scatter(\n",
    "            sorted_time,\n",
    "            sorted_effects,\n",
    "            alpha=0.3,\n",
    "            s=10\n",
    "        ) # Removed label for points\n",
    "\n",
    "        # Ensure the intervals are 1D arrays and sort them\n",
    "        lower_ci = np.ravel(effects_intervals[0])[sort_idx]\n",
    "        upper_ci = np.ravel(effects_intervals[1])[sort_idx]\n",
    "        # Add label for Confidence Interval\n",
    "        ax1.fill_between(\n",
    "            sorted_time,\n",
    "            lower_ci,\n",
    "            upper_ci,\n",
    "            alpha=0.2,\n",
    "            color='blue',\n",
    "            label='95% CI'\n",
    "        )\n",
    "\n",
    "        # Add trend line\n",
    "        # Ensure sorted_effects is 1D\n",
    "        sorted_effects_1d = np.ravel(sorted_effects)\n",
    "        z = np.polyfit(sorted_time, sorted_effects_1d, 3)  # Cubic fit\n",
    "        p = np.poly1d(z)\n",
    "        x_range = np.linspace(min(sorted_time), max(sorted_time), 100)\n",
    "        ax1.plot(x_range, p(x_range), \"r--\", label=\"Trend (Polyfit)\", linewidth=2)\n",
    "        ax1.axhline(\n",
    "            y=0,\n",
    "            color='gray',\n",
    "            linestyle='--',\n",
    "            alpha=0.7,\n",
    "            label='Zero Effect' # Added label\n",
    "        )\n",
    "        ax1.axhline(\n",
    "            y=ate,\n",
    "            color='red',\n",
    "            linestyle='-',\n",
    "            alpha=0.7,\n",
    "            label=f'ATE: {ate:.4f} (p={p_value:.4f})'\n",
    "        )\n",
    "\n",
    "        ax1.set_title(\n",
    "            f\"Temporal Effect $\\operatorname{{CATE}}$ on {outcome_name} by {treatment_name}\",\n",
    "            fontsize=14\n",
    "        )\n",
    "        ax1.set_xlabel(\"$\\operatorname{Day}$\", fontsize=12)\n",
    "        ax1.set_ylabel(\"$\\operatorname{CATE}$\", fontsize=12)\n",
    "\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend(fontsize=10) # Unified legend call\n",
    "        plt.tight_layout()\n",
    "        figures.append(fig1)\n",
    "\n",
    "        # ---------- 2. CATE by Month ----------\n",
    "        fig2, ax2 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        # Calculate effects by month\n",
    "        month_effects = []\n",
    "        month_errors = []\n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        valid_months = []\n",
    "\n",
    "        # First handle January (which is typically the reference month in dummy encoding)\n",
    "        is_jan = np.ones(len(X), dtype=bool)\n",
    "        for col in X.columns:\n",
    "            if col.startswith('month_'):\n",
    "                is_jan = is_jan & (X[col] == 0)\n",
    "\n",
    "        if np.any(is_jan):\n",
    "            month_effects.append(np.mean(effects[is_jan]))\n",
    "            month_errors.append(np.std(effects[is_jan]) / np.sqrt(np.sum(is_jan)))\n",
    "            valid_months.append('Jan')\n",
    "\n",
    "        # Handle other months\n",
    "        for i, month in enumerate(month_names[1:], 2):  # Start from February (2)\n",
    "            col = f'month_{i}'\n",
    "            if col in X.columns:\n",
    "                is_month = X[col] == 1\n",
    "            if np.any(is_month):\n",
    "                month_effects.append(np.mean(effects[is_month]))\n",
    "                month_errors.append(\n",
    "                np.std(effects[is_month]) / np.sqrt(np.sum(is_month))\n",
    "                )\n",
    "                valid_months.append(month)\n",
    "\n",
    "        # Plot as bar chart with error bars\n",
    "        x_pos = np.arange(len(valid_months))\n",
    "        # Added label for bars (representing mean effect per month)\n",
    "        ax2.bar(\n",
    "            x_pos,\n",
    "            month_effects,\n",
    "            yerr=month_errors,\n",
    "            align='center',\n",
    "            alpha=0.7,\n",
    "            color='skyblue',\n",
    "            ecolor='black',\n",
    "            capsize=10,\n",
    "            label='Monthly Mean CATE (Error Bars: SE)' # Label for bars and error bars\n",
    "        )\n",
    "        ax2.set_xticks(x_pos)\n",
    "        ax2.set_xticklabels(valid_months)\n",
    "\n",
    "        ax2.set_title(\n",
    "            f\"Seasonal Effects $\\operatorname{{CATE}}$ on {outcome_name} by {treatment_name}\",\n",
    "            fontsize=14\n",
    "        )\n",
    "        ax2.set_ylabel('$\\operatorname{CATE}$', fontsize=12)\n",
    "        ax2.set_xlabel('$\\operatorname{isMonth}$', fontsize=12)\n",
    "        ax2.axhline(\n",
    "            y=0,\n",
    "            color='gray',\n",
    "            linestyle='--',\n",
    "            alpha=0.7,\n",
    "            label='Zero Effect' # Added label\n",
    "        )\n",
    "        ax2.axhline(\n",
    "            y=ate,\n",
    "            color='red',\n",
    "            linestyle='-',\n",
    "            alpha=0.7,\n",
    "            label=f'ATE: {ate:.4f} (p={p_value:.4f})'\n",
    "        )\n",
    "        ax2.grid(alpha=0.3)\n",
    "        ax2.legend(fontsize=10) # Unified legend call\n",
    "        plt.tight_layout()\n",
    "        figures.append(fig2)\n",
    "\n",
    "        # ---------- 3. CATE by Treatment ----------\n",
    "        fig3, ax3 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        # Clean T by replacing infinite values with NaN\n",
    "        T_array = np.array(T)\n",
    "\n",
    "        # Sort T_filtered and effects_filtered based on T_filtered values\n",
    "        sort_idx_t = np.argsort(T_array)\n",
    "        sorted_T = T_array[sort_idx_t]\n",
    "        sorted_effects_t = effects[sort_idx_t]\n",
    "\n",
    "        # Plot CATE vs Treatment line and points\n",
    "        ax3.plot(\n",
    "            sorted_T,\n",
    "            sorted_effects_t,\n",
    "            linewidth=1.5,\n",
    "            alpha=0.8,\n",
    "            color='darkgreen',\n",
    "            label='$\\operatorname{CATE}$' # Updated label\n",
    "        )\n",
    "        ax3.scatter(\n",
    "            sorted_T,\n",
    "            sorted_effects_t,\n",
    "            alpha=0.4,\n",
    "            s=10,\n",
    "            color='green'\n",
    "        ) # Removed label for points\n",
    "\n",
    "        # Get and plot confidence intervals for CATE (using original effect inference)\n",
    "        lower_ci_t = np.ravel(effects_intervals[0])[sort_idx_t]\n",
    "        upper_ci_t = np.ravel(effects_intervals[1])[sort_idx_t]\n",
    "\n",
    "        # Plot confidence intervals with label\n",
    "        ax3.fill_between(\n",
    "            sorted_T,\n",
    "            lower_ci_t,\n",
    "            upper_ci_t,\n",
    "            alpha=0.2,\n",
    "            color='mediumseagreen',\n",
    "            label='95% CI' # Updated label\n",
    "        )\n",
    "\n",
    "        # Add a trend line\n",
    "        # Using polynomial fit on sorted data\n",
    "        sorted_effects_t_1d = np.ravel(sorted_effects_t)\n",
    "        # Calculate polynomial coefficients for cubic fit\n",
    "        z_t = np.polyfit(sorted_T, sorted_effects_t_1d, 3)\n",
    "        # Create polynomial function\n",
    "        p_t = np.poly1d(z_t)\n",
    "        # Generate x-values for the trend line\n",
    "        t_range = np.linspace(min(sorted_T), max(sorted_T), 100)\n",
    "        # Plot the trend line\n",
    "        ax3.plot(t_range, p_t(t_range), \"r--\", label=\"Trend (Polyfit)\", linewidth=2)\n",
    "\n",
    "        # Add ATE and zero lines\n",
    "        ax3.axhline(\n",
    "            y=0,\n",
    "            color='gray',\n",
    "            linestyle='--',\n",
    "            alpha=0.7,\n",
    "            label='Zero Effect' # Added label\n",
    "        )\n",
    "        ax3.axhline(\n",
    "            y=ate,\n",
    "            color='red',\n",
    "            linestyle='-',\n",
    "            alpha=0.7,\n",
    "            label=f'ATE: {ate:.4f} (p={p_value:.4f})'\n",
    "        )\n",
    "\n",
    "        # Set title and labels\n",
    "        ax3.set_title(\n",
    "            f\"Effect $\\operatorname{{CATE}}$ on {outcome_name} by {treatment_name}\", # Updated title\n",
    "            fontsize=14\n",
    "        )\n",
    "        ax3.set_xlabel(f'$T$: {treatment_name}', fontsize=12)\n",
    "        ax3.set_ylabel('$\\operatorname{CATE}$', fontsize=12) # Updated Y label\n",
    "\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.legend(fontsize=10) # Unified legend call\n",
    "        plt.tight_layout()\n",
    "        figures.append(fig3)\n",
    "\n",
    "        # ---------- 4. CATE Distribution ----------\n",
    "        fig4, ax4 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        # Ensure effects is 1D for histogram\n",
    "        effects_1d = np.ravel(effects)\n",
    "\n",
    "        # Plot histogram with kernel density estimate\n",
    "        sns.histplot(\n",
    "            effects_1d,\n",
    "            kde=True,\n",
    "            ax=ax4,\n",
    "            bins=30,\n",
    "            color='purple',\n",
    "            alpha=0.6\n",
    "        )\n",
    "\n",
    "        # Add a rug plot to show individual effects\n",
    "        sns.rugplot(\n",
    "            effects_1d,\n",
    "            ax=ax4,\n",
    "            color='black',\n",
    "            alpha=0.3\n",
    "        )\n",
    "\n",
    "        ax4.set_title(\n",
    "            f\"Distribution of Effects $\\operatorname{{CATE}}(Z)$ of {treatment_name} on {outcome_name}\",\n",
    "            fontsize=14\n",
    "        )\n",
    "        ax4.set_xlabel(\"$\\operatorname{CATE}(Z)$\", fontsize=12)\n",
    "        ax4.set_ylabel(\"$\\hat{p}_n \\left( \\operatorname{CATE}(Z) \\\\right)$\", fontsize=12)\n",
    "\n",
    "        ax4.axvline(\n",
    "            x=ate,\n",
    "            color='red',\n",
    "            linestyle='-',\n",
    "            linewidth=2,\n",
    "            label=f'ATE: {ate:.4f} (p={p_value:.4f})'\n",
    "        )\n",
    "        ax4.axvline(\n",
    "            x=0,\n",
    "            color='gray',\n",
    "            linestyle='--',\n",
    "            alpha=0.7,\n",
    "            label='Zero Effect' # Added label for the zero line\n",
    "        )\n",
    "\n",
    "        ax4.grid(alpha=0.3)\n",
    "        ax4.legend() # Display the legend with labels for ATE and Zero Effect\n",
    "        plt.tight_layout()\n",
    "        figures.append(fig4)\n",
    "\n",
    "        # ---------- 5. Treatment vs. Outcome by Season ----------\n",
    "        fig5, ax5 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        # Explicitly replace infinity values with NaN\n",
    "        Y_array = np.array(Y)\n",
    "\n",
    "        # Divide time into quarters\n",
    "        q_labels = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
    "        time_quartiles = pd.qcut(X['days_sequence'], 4, labels=q_labels)\n",
    "\n",
    "        # Create a colormap\n",
    "        cmap = plt.cm.viridis\n",
    "        colors = cmap(np.linspace(0, 1, 4))\n",
    "\n",
    "        # Plot each quartile with different colors\n",
    "        for i, (label, color) in enumerate(zip(q_labels, colors)):\n",
    "            mask = time_quartiles == label\n",
    "            ax5.scatter(\n",
    "            T_array[mask],\n",
    "            Y_array[mask],\n",
    "            alpha=0.6,\n",
    "            color=color,\n",
    "            label=label # Label for scatter points (used in legend)\n",
    "            )\n",
    "\n",
    "        # Add regression lines for each quartile\n",
    "        for i, (label, color) in enumerate(zip(q_labels, colors)):\n",
    "            mask = time_quartiles == label\n",
    "            # Filter out potential NaNs or Infs before polyfit\n",
    "            z = np.polyfit(T_array[mask], Y_array[mask], 1)\n",
    "            p = np.poly1d(z)\n",
    "            # Define x_range based on valid data for the quartile\n",
    "            t_min, t_max = np.min(T_array[mask]), np.max(T_array[mask])\n",
    "            x_range = np.linspace(t_min, t_max, 20)\n",
    "            # Plot regression line - no separate label, associated by color\n",
    "            ax5.plot(x_range, p(x_range), color=color, linestyle='--')\n",
    "\n",
    "        ax5.set_title(\n",
    "            f\"{treatment_name}-{outcome_name} Relationship: $Y = \\\\theta(Z) T + g(X) + \\epsilon_Y$ by Season\",\n",
    "            fontsize=14\n",
    "        )\n",
    "\n",
    "        ax5.set_xlabel(f\"${{T}}$: {treatment_name}\", fontsize=12)\n",
    "        ax5.set_ylabel(f\"${{Y}}$: {outcome_name}\", fontsize=12)\n",
    "\n",
    "        ax5.grid(alpha=0.3)\n",
    "        # Legend will show labels from scatter plot ('Q1', 'Q2', 'Q3', 'Q4')\n",
    "        ax5.legend(title=\"Time Period\")\n",
    "        plt.tight_layout()\n",
    "        figures.append(fig5)\n",
    "\n",
    "        # ---------- 6. Feature Importance for CATE ----------\n",
    "        #try:\n",
    "        feature_importances = model.feature_importances()\n",
    "\n",
    "            # Ensure feature_importances is 1D\n",
    "            #if len(feature_importances.shape) > 1:\n",
    "                #feature_importances = feature_importances.flatten()\n",
    "\n",
    "            # Only plot if we have feature importances\n",
    "            #if len(feature_importances) > 0:\n",
    "        fig6, ax6 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        # Get feature names\n",
    "        feature_names = X.columns.tolist()\n",
    "\n",
    "        ## Ensure lengths match\n",
    "        #min_length = min(len(feature_importances), len(feature_names))\n",
    "        #valid_features = feature_importances[:min_length]\n",
    "        #valid_names = feature_names[:min_length]\n",
    "\n",
    "        # Sort feature importances (ascending)\n",
    "        sort_idx = np.argsort(feature_importances)\n",
    "\n",
    "        # Take at most 10 most important features\n",
    "        if len(sort_idx) > 10:\n",
    "            sort_idx = sort_idx[-10:]\n",
    "\n",
    "        # Get sorted values\n",
    "        sorted_importances = feature_importances[sort_idx]\n",
    "        sorted_feature_names = [feature_names[i] for i in sort_idx]\n",
    "\n",
    "        # Plot horizontal bar chart\n",
    "        ax6.barh(\n",
    "            range(len(sorted_importances)),\n",
    "            sorted_importances, align='center',\n",
    "            color='teal'\n",
    "        )\n",
    "        ax6.set_yticks(range(len(sorted_importances)))\n",
    "        ax6.set_yticklabels(sorted_feature_names)\n",
    "\n",
    "        ax6.set_title(\"Feature Importance for $\\mathsf{CauslForest}$\", fontsize=14)\n",
    "        ax6.set_xlabel(\"Importance\", fontsize=12)\n",
    "        ax6.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        figures.append(fig6)\n",
    "        #except Exception as e:\n",
    "            #print(f\"Could not plot feature importance: {e}\")\n",
    "\n",
    "        return figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#from Modeler import Modeler\n",
    "\n",
    "mode = input('Dataset is for train or test?')\n",
    "mol = Modeler(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "handling factors datas missing values progress: 100%|██████████| 7/7 [00:00<00:00, 40.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factors_data': [],\n",
       " 'fundamentals_data': [],\n",
       " 'macros_data': [],\n",
       " 'money_flows_data': [],\n",
       " 'securities_margins_data': [],\n",
       " 'industries_data': [],\n",
       " 'indexes_data': []}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol.check_factor_data_nan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mol.industry_factors_datas = pickle.load(open('../data/modeled_data/bank_factors_datas_train.pkl', mode='rb+'))\n",
    "except:\n",
    "    mol.industry_factors_datas = mol.clean_and_average_factors_datas()\n",
    "    pickle.dump(obj=mol.industry_factors_datas, file=open(file='../data/modeled_data/bank_factors_datas_train.pkl', mode='wb+'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mol.industry_r_wacc_data = pickle.load(open('../data/modeled_data/bank_r_wacc_data_train.pkl', mode='rb+'))\n",
    "except:\n",
    "    mol.industry_r_wacc_data = mol.clean_and_average_r_wacc_data()\n",
    "    pickle.dump(obj=mol.industry_r_wacc_data, file=open(file='../data/modeled_data/bank_r_wacc_data_train.pkl', mode='wb+'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mol.industry_factors_df = pd.read_csv('../data/modeled_data/bank_factors_df_train.csv', index_col='date', parse_dates=['date'])\n",
    "    mol.factors_risks_data_standardized = pd.read_csv('../data/dict/factors_risks_dicts_standardized.csv', index_col='factor_code')\n",
    "except:\n",
    "    mol.industry_factors_df = mol.transform_factors_datas_from_dict_to_df()\n",
    "    mol.factors_risks_data_standardized = mol.standardize_factors_risks_data()\n",
    "    mol.industry_factors_df.to_csv('../data/modeled_data/bank_factors_df_train.csv', encoding='utf-8')\n",
    "    mol.factors_risks_data_standardized.to_csv('../data/dict/factors_risks_dicts_standardized.csv', encoding='utf-8', index=True, index_label='factor_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_type = 'risk_factors_clusters'\n",
    "filename = rf\"../image/{fig_type}.png\"\n",
    "\n",
    "try:\n",
    "    # 尝试从文件加载图像\n",
    "    fig = mpimg.imread(filename)\n",
    "    print(f\"Loaded figure {fig_type} from {filename}\")\n",
    "\n",
    "    # 显示加载的图像\n",
    "    plt.figure(figsize=(20, 15), dpi=300)\n",
    "    plt.imshow(fig)\n",
    "except FileNotFoundError:\n",
    "    # 创建新图形并保存\n",
    "    fig = mol.visualize_risk_factors_clusters(random_seed=52)\n",
    "    # 确保保存格式和文件扩展名匹配\n",
    "    fig.savefig(filename, dpi=300)\n",
    "    print(f\"Saved figure {fig_type} to {filename}\")\n",
    "\n",
    "    # 直接显示创建的图形\n",
    "    plt.figure(fig.number)  # 激活这个图形窗口\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "plt.show()  # 确保图形显示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mol.industry_risks_premiums_totals = pickle.load(open('../data/modeled_data/bank_risks_premiums_totals.pkl', mode='rb+'))\n",
    "    mol.risks_premiums_totals_curs = pickle.load(open('../model/risks_premiums_totals_curs.pkl', mode='rb+'))\n",
    "except:\n",
    "    mol.industry_risks_premiums_totals, mol.risks_premiums_totals_curs = mol.get_industry_risks_premiums_totals()\n",
    "    pickle.dump(obj=mol.industry_risks_premiums_totals, file=open(file='../data/modeled_data/bank_risks_premiums_totals.pkl', mode='wb+'), protocol=4)\n",
    "    pickle.dump(obj=mol.risks_premiums_totals_curs, file=open(file='../model/risks_premiums_totals_curs.pkl', mode='wb+'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mol.industry_risks_premiums_components = pickle.load(open('../data/modeled_data/bank_risks_premiums_components.pkl', mode='rb+'))\n",
    "    mol.risks_premiums_components_curs = pickle.load(open('../model/risks_premiums_components_curs.pkl', mode='rb+'))\n",
    "except:\n",
    "    mol.industry_risks_premiums_components, mol.risks_premiums_components_curs = mol.get_industry_risks_premiums_components()\n",
    "    pickle.dump(obj=mol.industry_risks_premiums_components, file=open(file='../data/modeled_data/bank_risks_premiums_components.pkl', mode='wb+'), protocol=4)\n",
    "    pickle.dump(obj=mol.risks_premiums_components_curs, file=open(file='../model/risks_premiums_components_curs.pkl', mode='wb+'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mol.causal_forest_forests = pickle.load(open('../model/bank_causal_forest_forests.pkl', mode='rb+'))\n",
    "except:\n",
    "    mol.causal_forest_forests = mol.estimate_causal_effects_with_dml()\n",
    "    pickle.dump(obj=mol.causal_forest_forests, file=open(file='../model/bank_causal_forest_forests.pkl', mode='wb+'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_type = 'causal_forest_trees'\n",
    "filename = rf\"../image/causal_effect/{fig_type}.png\"\n",
    "\n",
    "try:\n",
    "    # 尝试从文件加载图像\n",
    "    fig = mpimg.imread(filename)\n",
    "\n",
    "    # 显示加载的图像\n",
    "    plt.figure(figsize=(20, 15), dpi=300)\n",
    "    plt.imshow(fig)\n",
    "    print(f\"Loaded figure {fig_type} from {filename}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # 创建新图形并保存\n",
    "    fig = mol.visualize_causal_forest_trees()\n",
    "    # 确保保存格式和文件扩展名匹配\n",
    "    fig.savefig(filename, dpi=300)\n",
    "\n",
    "    # 直接显示创建的图形\n",
    "    plt.figure(fig.number)  # 激活这个图形窗口\n",
    "    print(f\"Saved figure {fig_type} to {filename}\")\n",
    "\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "plt.show()  # 确保图形显示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all figures to files with descriptive names\n",
    "fig_types = [\n",
    "    'cate_over_time',\n",
    "    'cate_by_month',\n",
    "    'cate_by_treatment',\n",
    "    'ate_distribution',\n",
    "    'treatment_vs_outcome',\n",
    "    'causal_forest_feature_importance'\n",
    "]\n",
    "n_types = len(fig_types)\n",
    "\n",
    "for treatment in ['default_risk', 'liquidity_risk', 'market_risk']:\n",
    "    for outcome in ['common_risk', 'idiosyncratic_risk']:\n",
    "        try:\n",
    "            for fig_type in fig_types:\n",
    "                filename = rf\"../image/causal_effect/{fig_type}_{treatment}_vs_{outcome}.png\"\n",
    "                # 尝试从文件加载图像\n",
    "                fig = mpimg.imread(filename)\n",
    "                print(f\"Loaded figure {fig_type} from {filename}\")\n",
    "\n",
    "                # 显示加载的图像\n",
    "                plt.figure(figsize=(20, 15), dpi=300)\n",
    "                plt.imshow(fig)\n",
    "\n",
    "                plt.axis('off')\n",
    "                plt.tight_layout(pad=0)\n",
    "                plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "                plt.show()  # 确保图形显示\n",
    "        except:\n",
    "            figs = mol.visualize_causal_effect(treatment, outcome)\n",
    "            for fig, fig_type in zip(figs, fig_types):\n",
    "                filename = rf\"../image/causal_effect/{fig_type}_{treatment}_vs_{outcome}.png\"\n",
    "\n",
    "                fig.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "                print(f\"Saved figure {fig} to {filename}\")\n",
    "\n",
    "            # 直接显示创建的图形\n",
    "            plt.figure(fig.number)  # 激活这个图形窗口\n",
    "\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout(pad=0)\n",
    "            plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "            plt.show()  # 确保图形显示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

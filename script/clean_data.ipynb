{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "class CustomRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, mode, random_state, index,\n",
    "    dt_params_dist_1={\n",
    "        'criterion': ['friedman_mse', 'absolute_error'],\n",
    "        'max_depth': Integer(2, 10), \n",
    "        'min_samples_split': Integer(2, 20),\n",
    "        'max_features': Categorical(['sqrt', 'log2', None]),\n",
    "    },\n",
    "    ada_params_dist={\n",
    "        'loss': ['linear', 'square'],\n",
    "        'n_estimators': Integer(10, 100),\n",
    "        'learning_rate': Real(1e-2, 1e0, prior='log-uniform'),\n",
    "    },\n",
    "    dt_params_dist_2={\n",
    "        'criterion': ['friedman_mse', 'absolute_error'],\n",
    "        'max_depth': Integer(2, 5), # 用于蒸馏ada2，因此应该浅一些\n",
    "        'min_samples_split': Integer(2, 20),\n",
    "        'max_features': Categorical(['sqrt', 'log2', None]), #\n",
    "    },\n",
    "    bayes_search_param={\n",
    "        'n_iter': 20,\n",
    "        'n_points': 5, # 使用比较大的n_initial_points参数可以显著减少\"The objective has been evaluated at this point before\"的警告\n",
    "        'cv': 5, \n",
    "        'scoring': 'neg_mean_squared_error', \n",
    "        'n_jobs': -1,\n",
    "        'verbose': 1\n",
    "    },\n",
    "    RFECV_param={\n",
    "    'estimator': None, \n",
    "    'step': 0.05, \n",
    "    'cv': 5, \n",
    "    'scoring': 'neg_mean_squared_error',\n",
    "    'min_features_to_select': 10\n",
    "    }\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.random_state = random_state\n",
    "        self.index = index\n",
    "        \n",
    "        # trian模式需要训练，从而需要定义用于训练的模型\n",
    "        if self.mode == 'train':\n",
    "            # 定义各模型的超参数区间及搜索模型\n",
    "            self.dt_params_dist_1 = dt_params_dist_1\n",
    "            self.ada_params_dist=ada_params_dist\n",
    "            self.dt_params_dist_2 = dt_params_dist_2\n",
    "            self.bayes_search_param = bayes_search_param\n",
    "            self.RFECV_param = RFECV_param\n",
    "\n",
    "            # 定义模型\n",
    "            self.dt = DecisionTreeRegressor(random_state=self.random_state)\n",
    "            self.ada = AdaBoostRegressor(estimator=None, random_state=self.random_state)\n",
    "\n",
    "            self.bayes_search = BayesSearchCV(\n",
    "                estimator=self.dt, # 形式上的参数，fit()中会更改，相当于None\n",
    "                search_spaces=self.dt_params_dist_1, # 形式上的参数，fit()中会更改，相当于None\n",
    "                random_state=self.random_state\n",
    "                ).set_params(self.bayes_search_param)\n",
    "            \n",
    "            self.rfecv = RFECV().set_params(self.RFECV_param)\n",
    "\n",
    "        # test模式直接输入train模式得到的模型训练\n",
    "        elif self.mode == 'test':\n",
    "            pass\n",
    "\n",
    "    def fit(self, X, y, ada_best=None):\n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            y = y.to_numpy()\n",
    "\n",
    "            # 如果传入的y是二维数组，大小为(m, 1)\n",
    "            if len(y.shape) == 2:\n",
    "            # 将其转化为一维数组，大小为(m,)\n",
    "                y = y.ravel()\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            # 传递模型的pipeline\n",
    "            # self.model在fit前都需要转变为clone(self.model)防止__init__中的self.model被链接并拟合\n",
    "            # 寻找最优决策树\n",
    "            dt_1_bayes_search = clone(self.bayes_search).set_params(estimator=clone(self.dt), search_spaces=self.dt_param_dist_1) \n",
    "            # 将一个估计器（如决策树dt）传递给BayesSearchCV的estimator参数时，在BayesSearchCV拟合后，这个估计器也会被拟合，并被设置为性能最好的超参数组合下拟合得到的决策树\n",
    "            # 所以也需要对dt进行clone，防止__init__中的self.dt被链接并拟合\n",
    "            dt_1_bayes_search.fit(X, y)\n",
    "            dt_1_best = dt_1_bayes_search.best_estimator_\n",
    "\n",
    "            # 将最优决策树输入adaboost，寻找最优adaboost\n",
    "            ada_1 = clone(self.ada).set_params(estimator=dt_1_best)\n",
    "            ada_1_bayes_search = clone(self.bayes_search).set_params(estimator=ada_1, search_spaces=self.ada_param_dist)\n",
    "            ada_1_bayes_search.fit(X, y)\n",
    "            self.ada_best = ada_1_bayes_search.best_estimator_\n",
    "\n",
    "            if X.shape[1] >= 2:\n",
    "                rfecv = clone(self.rfecv).set_params(estimator=self.ada_best)\n",
    "                rfecv.fit(X, y)\n",
    "                X_selected = rfecv.transform(X)\n",
    "\n",
    "                # 将最优特征子集输入决策树，寻找最优决策树\n",
    "                dt_2_bayes_search = clone(self.bayes_search).set_params(estimator=clone(self.dt), search_spaces=self.dt_param_dist_2)\n",
    "                dt_2_bayes_search.fit(X_selected, y)\n",
    "                dt_2_best = dt_2_bayes_search.best_estimator_\n",
    "\n",
    "                # 将最优决策树输入adaboost，寻找最优adaboost\n",
    "                ada_2 = clone(self.ada).set_params(estimator=dt_2_best)\n",
    "                ada_2_bayes_search = clone(self.bayes_search).set_params(estimator=ada_2, search_spaces=self.ada_param_dist)\n",
    "                ada_2_bayes_search.fit(X_selected, y)\n",
    "                self.ada_best = ada_2_bayes_search.best_estimator_\n",
    "                \n",
    "        elif self.mode == 'test':\n",
    "            self.ada_best = ada_best\n",
    "            \n",
    "        return self # 为了链式调用，即CustomRegressor.fit(X, y).xxx == self.xxx\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = pd.DataFrame(self.ada_best.predict(X))\n",
    "        y_pred.index = self.index\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    # analog fit_transform from transformer\n",
    "    def fit_predict(self, X, y, ada_best=None):\n",
    "        self.fit(X, y, ada_best)\n",
    "        y_pred = self.predict(X)\n",
    "\n",
    "        return y_pred, self.ada_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import xgboost as xgb\n",
    "\n",
    "class Modeler():\n",
    "\n",
    "    def __init__(self, mode='train', factors_datas_names=[\n",
    "        'factors_data',\n",
    "        'fundamentals_data',\n",
    "        'macros_data',\n",
    "        'money_flows_data',\n",
    "        'securities_margins_data',\n",
    "        'industries_data',\n",
    "        'indexes_data'\n",
    "    ],\n",
    "    other_datas_names=[\n",
    "        'bank_stocks_info',\n",
    "        'returns_data',\n",
    "        'FCF_discounted_model_params_data'\n",
    "    ],\n",
    "    random_state=20240301\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.random_state = random_state\n",
    "        # 将所有factors_datas以外的数据都定义为类属性\n",
    "        for other_data_name in other_datas_names:\n",
    "            other_data = pickle.load(open(f'{other_data_name}_{mode}.pkl', mode='rb+'))\n",
    "            if other_data_name == 'bank_stocks_info':\n",
    "                self.industry_stocks_info = other_data\n",
    "            elif other_data_name == 'returns_data':\n",
    "                self.returns_data = other_data\n",
    "            elif other_data_name == 'FCF_discounted_model_params_data':\n",
    "                self.FCF_discounted_model_params_data = other_data\n",
    "\n",
    "        self.index = self.FCF_discounted_model_params_data['r_wacc'].index\n",
    "        self.columns = self.FCF_discounted_model_params_data['r_wacc'].columns\n",
    "\n",
    "        # 合并多个因子数据表\n",
    "        factors_datas = {}\n",
    "        # 导入数据\n",
    "        for factors_data_name in factors_datas_names:\n",
    "            factors_data = pickle.load(open(f'{factors_data_name}_{mode}.pkl', mode='rb+'))\n",
    "            factors_datas[factors_data_name] = factors_data\n",
    "        \n",
    "        self.factors_datas = factors_datas\n",
    "        pickle.dump(obj=factors_datas, file=open(file=f'factors_datas_{self.mode}.pkl', mode='wb+'), protocol=4)\n",
    "\n",
    "    # 检查各因子是否为空表\n",
    "    def check_factor_data_nan(self):\n",
    "        factor_data_nan_dict = {}\n",
    "        for factors_data_name, factors_data in tqdm(self.factors_datas.items(), desc='handling factors datas missing values progress'):\n",
    "            factor_data_nan_dict[factors_data_name] = []\n",
    "            for factor_data_name, factor_data in factors_data.items():\n",
    "                if not factor_data.any().any():\n",
    "                    factor_data_nan_dict[factors_data_name].append(factor_data_name)\n",
    "\n",
    "        return factor_data_nan_dict\n",
    "        \n",
    "\n",
    "    # 定义标准化因子风险归类数据表\n",
    "    def standardize_factors_risks_data(self):\n",
    "        # 导入手动划分的因子风险归类数据表factors_risks_data，其有三列，分别是因子中文名称、因子代码和因子所属风险\n",
    "        factors_risks_data = pd.read_csv('factors_risks_data.csv')\n",
    "\n",
    "        # 检查factors_risks_data中是否有缺失的因子代码\n",
    "        factors_codes_missing = []\n",
    "        for _, factors_data in self.factors_datas.items():\n",
    "            for factor_data_name in factors_data.keys():\n",
    "                if factor_data_name not in factors_risks_data['factor_code'].values:\n",
    "                    factors_codes_missing.append(factor_data_name)\n",
    "\n",
    "        # 检查各因子代码是否有重复值\n",
    "\n",
    "        # 定义因子代码列表\n",
    "        factors_codes = []\n",
    "        for factors_data in self.factors_datas.values():\n",
    "            for factor_data_name in factors_data.keys():\n",
    "                factors_codes.append(factor_data_name)\n",
    "\n",
    "        # 定义列表重复值查找函数\n",
    "        def find_duplicates(lst):\n",
    "            duplicates = []\n",
    "            unique_elements = set()\n",
    "\n",
    "            for item in lst:\n",
    "                if item in unique_elements:\n",
    "                    duplicates.append(item)\n",
    "                else:\n",
    "                    unique_elements.add(item)\n",
    "\n",
    "            return duplicates\n",
    "        \n",
    "        # 检查因子代码列表是否有重复值\n",
    "        factors_codes_duplicated = find_duplicates(factors_codes)\n",
    "        \n",
    "        # 检查factors_risks_data是否有多余的因子代码\n",
    "        factors_codes_excessive = []\n",
    "        for factor_code in factors_risks_data['factor_code'].values:\n",
    "            if factor_code not in factors_codes:\n",
    "                factors_codes_excessive.append(factor_code)\n",
    "\n",
    "        # 将factors_risks_data中的risk列标准化为0-1变量\n",
    "        factors_risks_data = factors_risks_data.set_index('factor_code')\n",
    "        factors_risks_data_standardized = pd.DataFrame(0, columns=['default_risk', 'liquidity_risk', 'market_risk'], index=factors_risks_data.index)\n",
    "        for factor_code in factors_risks_data.index.tolist():\n",
    "            if 'Default Risk' in factors_risks_data.loc[factor_code, 'risk']:\n",
    "                factors_risks_data_standardized.loc[factor_code, 'default_risk'] = 1\n",
    "            if 'Liquidity Risk' in factors_risks_data.loc[factor_code, 'risk']:\n",
    "                factors_risks_data_standardized.loc[factor_code, 'liquidity_risk'] = 1\n",
    "            if 'Market Risk' in factors_risks_data.loc[factor_code, 'risk']: \n",
    "                factors_risks_data_standardized.loc[factor_code, 'market_risk'] = 1\n",
    "\n",
    "        self.factors_risks_data_standardized = factors_risks_data_standardized\n",
    "        \n",
    "        return factors_codes_missing, factors_codes_excessive, factors_codes_duplicated, factors_risks_data_standardized\n",
    "    \n",
    "    def _fill_nan_col(self, factor_data_without_type1_missing, factor_data):\n",
    "        #print('factor_data_without_type1_missing', factor_data_without_type1_missing)\n",
    "        # 找到factor_data_without_type1_missing中全为缺失值的列名\n",
    "        missing_stocks_codes = factor_data_without_type1_missing.columns[factor_data_without_type1_missing.isnull().all()]\n",
    "        #print('missing_stocks_codes', missing_stocks_codes)\n",
    "        # 导出对应列在factor_data_without_type1_missing各行的分位数数据表，索引为缺失值列名\n",
    "        missing_stocks_codes_quantiles = factor_data.rank(pct=True, axis=1)[missing_stocks_codes]\n",
    "        # 计算对应列在factor_data_without_type1_missing各行的分位数据表的平均值series，索引为缺失值列名\n",
    "        missing_stocks_codes_quantiles_mean = missing_stocks_codes_quantiles.mean()\n",
    "        #print('missing_stocks_codes_quantiles_mean', missing_stocks_codes_quantiles_mean)\n",
    "        # fill_values是factor_data_without_type1_missing中全为缺失值的各列在对应分位数平均值处的值series，索引为列名\n",
    "        fill_values = factor_data_without_type1_missing.quantile(missing_stocks_codes_quantiles_mean, axis=1)\n",
    "\n",
    "        # 兼容fill_values与factor_data_without_type1_missing\n",
    "        fill_values = fill_values.T\n",
    "        fill_values.columns = missing_stocks_codes\n",
    "\n",
    "        # 用以上series填充对应缺失值列的缺失值\n",
    "        factor_data_without_type1_missing.update(fill_values) # 不可将df赋给df，前者这样应该取values，变为np.array，但是还是会warning，所以使用update\n",
    "        #print('fill_values', fill_values)\n",
    "        #print('factor_data_without_type1_missing_filled', factor_data_without_type1_missing)\n",
    "\n",
    "        return factor_data_without_type1_missing\n",
    "\n",
    "    # 缺失值处理\n",
    "    def _handle_missing_values(self, factor_data):\n",
    "        '''factor_data中有些因子值缺失，而这样缺失值要么是由于个股在上市前或者退市后，因子值不存在；要么是因为个股在市期间，其因子值没有被披露或被统计。在进行缺失值\n",
    "        处理时，忽略前一种缺失值，而填充后一种缺失值。\n",
    "            填充缺失值一般有三种方法，即SimpleImputation，KNNImputation和IterativeImputation。对于本面板数据，SimpleImputation（如均值填充或中位数填充）可能不\n",
    "        适合，因为它没有考虑时间序列的特性和个股之间的相关性。简单地用一个常数填充缺失值可能会引入偏差，尤其是当缺失值的比例较高时。而KNNImputation可以考虑个股之间的\n",
    "        相关性，但它也没有考虑时间序列的特性。此外，KNNImputation在处理大规模面板数据时可能会比较慢，因为它需要计算所有个股之间的距离矩阵。所以应该选择IterativeImpu\n",
    "        -tion。优缺点：考虑时间序列的特性，个股之间的相关性。\n",
    "            然而，在使用IterativeImputer填充所有缺失值后再删除第一类缺失值可能不是最佳方案。这是因为IterativeImputer在估计缺失值时会考虑所有的特征，包括那些本不应\n",
    "        该存在和被填充的第一类缺失值。这可能会影响估计的质量。\n",
    "            所以，以下方案是更好的选择。首先，识别出那些不包含第一类缺失值的样本日期，并仅使用这些样本来训练IterativeImputer。然后，使用训练后的IterativeImputer来\n",
    "        估计所有样本中的第二类缺失值。'''\n",
    "        # 定义一个掩码mask,标识每个个股在每个时间点上是否处于上市状态\n",
    "        mask = pd.DataFrame(index=self.index, columns=self.columns)\n",
    "        # 传出因子数据表每一行axis=1的中位数，组成各截面的中位数向量meidians\n",
    "        medians = factor_data.median(axis=1)\n",
    "        # 对于每个代码为stock的个股\n",
    "        for stock_code in mask.columns:\n",
    "            # 对于代码为stock_code的个股，查找板块个股代码列表industry_stocks_info对应的个股代码，传出其上市日期start_date和退市日日期end_date\n",
    "            start_date = self.industry_stocks_info.loc[stock_code, 'start_date']\n",
    "            end_date = self.industry_stocks_info.loc[stock_code, 'end_date']\n",
    "            # 标识代码为stock的个股在每个时间点上是否处于上市状态\n",
    "            mask[stock_code] = (mask.index >= start_date) & (mask.index <= end_date)\n",
    "            \n",
    "            # 对于代码为stock_code的个股，如果其在各时间上的因子值factor_data[stock_code]均为缺失值np.nan\n",
    "            if factor_data[stock_code].isnull().all():\n",
    "                #print('factor_data', factor_data, 'stock_code', stock_code)\n",
    "                # 将此代码为stock_code的个股其在各时间上的因子值factor_data[stock_code]传为各截面的中位数向量meidians，防止被Imputer忽略\n",
    "                medians = pd.DataFrame(medians)\n",
    "                medians.columns = [stock_code]\n",
    "                factor_data.update(medians)\n",
    "                #print('factor_data', factor_data)\n",
    "        \n",
    "        # 根据掩码mask，传出不包含第一类缺失值(不在市)的日期索引indexes_without_type1_missing\n",
    "        indexes_without_type1_missing = mask.all(axis=1)\n",
    "        # 定义不包含第一类缺失值的日期索引indexes_without_type1_missing对应的因子数据样本factor_data_without_type1_missing\n",
    "        factor_data_without_type1_missing = factor_data.loc[indexes_without_type1_missing]\n",
    "\n",
    "        # 对于factor_data_without_type1_missing中全为缺失值的列，得到其在整个数据factor_data中的分位数平均值，取此分位数在factor_data_without_type1_missing中各行对应的值填充缺失值列\n",
    "        factor_data_without_type1_missing = self._fill_nan_col(factor_data_without_type1_missing, factor_data)\n",
    "        \n",
    "        # 定义IterativeImputer，所有缺失值被填充后需要再次加入训练，再次填充原有缺失值，直至缺失值收敛，这样的递归次数max_iter为10，随机种子random_state为self.random_state\n",
    "        imputer = IterativeImputer(\n",
    "            random_state=self.random_state,\n",
    "            # 使用IsolationXFBoost填充缺失值\n",
    "            estimator=xgb.XGBRegressor(),\n",
    "            max_iter=50,\n",
    "            tol=1e-3\n",
    "            )\n",
    "        \n",
    "        # 使用factor_data_without_type1_missing来训练IterativeImputer\n",
    "        imputer.fit(factor_data_without_type1_missing)\n",
    "        # 利用imputer填充factor_data的全部缺失值，传出为填充后因子数据表factor_data_imputed\n",
    "        factor_data_imputed = imputer.transform(factor_data)\n",
    "        factor_data_imputed = pd.DataFrame(factor_data_imputed)\n",
    "        factor_data_imputed.index = self.index\n",
    "        factor_data_imputed.columns = self.columns\n",
    "\n",
    "        # 将填充后因子数据表factor_data_imputed中的第一类缺失值重新标记为np.nan，即使用训练后的imputer来估计所有因子数据表中的第二类缺失值\n",
    "        factor_data_imputed[~mask] = np.nan\n",
    "\n",
    "        return factor_data_imputed\n",
    "    \n",
    "    # Fama-French-3分位数差值处理\n",
    "    def _process_ff3_quantile_difference(self, factor_data):\n",
    "        '''FF3处理形成截面股价收益率:\n",
    "        合理性:如果您的研究目的是探究因子对股价收益率的截面预测能力,并且假设股价收益率的截面分布与因子的截面分布相关,那么按照FF3处理形成截面股价收益率是合适的。\n",
    "        优点:这种方法能够消除股价收益率的极值影响,使得截面股价收益率的分布更加稳定,便于研究因子的预测能力。\n",
    "        缺点:这种方法忽略了个股市值的影响,可能无法反映市场整体的收益率变化。'''\n",
    "        panal_factor_data_quantiles = factor_data.quantile([0.3, 0.7], axis=1)\n",
    "        panal_factor_data = panal_factor_data_quantiles.loc[0.7] - panal_factor_data_quantiles.loc[0.3]\n",
    "        return panal_factor_data\n",
    "    \n",
    "    # 企业价值加权处理\n",
    "    def _average_by_enterprise_value(self, factor_data):\n",
    "        '''按个股市值加权形成截面股价收益率:\n",
    "        合理性:如果您的研究目的是探究因子对市场整体收益率的预测能力,并且假设个股的市值反映了其在市场中的重要性,那么按个股市值加权形成截面股价收益率是合适的。\n",
    "        优点:这种方法考虑了个股市值的影响,能够反映市场整体的收益率变化,更接近实际的投资组合收益。\n",
    "        缺点:这种方法可能受到大市值股票的主导,小市值股票的影响可能被掩盖。\n",
    "        就本文的银行板块研究目的来说，选择按个股市值加权形成截面股价收益率。'''\n",
    "        weighted_factor_data = self.enterprise_value_weights * factor_data\n",
    "        panal_factor_data = weighted_factor_data.sum(axis=1)\n",
    "\n",
    "        panal_factor_data = pd.DataFrame(panal_factor_data)\n",
    "        panal_factor_data.index = self.index\n",
    "        #panal_factor_data.columns = [factor_data_name] 不重置列名，防止与其他panal_factor_data运算时因为列名不一致而出现两行缺失值\n",
    "\n",
    "        return panal_factor_data\n",
    "\n",
    "    # 数据清理，即缺失值处理和Fama-French-3分位数差值处理\n",
    "    def clean_and_average_factors_datas(self):\n",
    "        self.enterprise_value_weights = self.FCF_discounted_model_params_data['panal_enterprise_value_weights']\n",
    "        industry_factors_datas = self.factors_datas.copy()\n",
    "\n",
    "        for factors_data_name, factors_data in self.factors_datas.items():\n",
    "            for factor_data_name, factor_data in tqdm(factors_data.items(), desc='handling factors data progress'):\n",
    "                if factors_data_name != 'macros_data': #factor_data_name == 'PEG': \n",
    "                    # 缺失值处理\n",
    "                    try:\n",
    "                        factor_data_imputed = self._handle_missing_values(factor_data)\n",
    "                    except:\n",
    "                        print(factor_data_name, factors_data_name)\n",
    "                        raise\n",
    "\n",
    "                    # 企业价值加权处理\n",
    "                    industry_factor_data = self._average_by_enterprise_value(factor_data_name, factor_data_imputed)\n",
    "                    # 对r_waac以enterprise_value在截面的权重加权求和求出enterprise_value加权r_wacc，\n",
    "                    #分别求出circulating_value加权r_E和Debts加权r_D，再以截面总circulating_value和总Debts在截面的总enterprise_value的权重加权得到enterprise_value加权r_wacc\n",
    "                    # 二者过程等价\n",
    "                    industry_factors_datas[factors_data_name][factor_data_name] = industry_factor_data\n",
    "                \n",
    "        self.industry_factors_datas = industry_factors_datas\n",
    "        \n",
    "        return industry_factors_datas\n",
    "    \n",
    "    def clean_and_average_r_waac_data(self):\n",
    "        self.enterprise_value_weights = self.FCF_discounted_model_params_data['panal_enterprise_value_weights']\n",
    "        r_waac = self.FCF_discounted_model_params_data['r_wacc']\n",
    "        industry_r_waac_data = {}\n",
    "\n",
    "        r_waac_imputed = self._handle_missing_values(r_waac)\n",
    "\n",
    "        industry_r_waac = self._average_by_enterprise_value(r_waac_imputed)\n",
    "\n",
    "        industry_r_waac_data['r_waac'] = industry_r_waac\n",
    "\n",
    "        self.industry_r_waac_data = industry_r_waac_data\n",
    "\n",
    "        return industry_r_waac_data\n",
    "\n",
    "    # 取得系统性风险和非系统性风险风险溢价\n",
    "    def get_common_and_idiosyncratic_risks_premium(self):\n",
    "        # 载入数据\n",
    "        r_wacc = self.industry_r_waac_data['r_waac']\n",
    "        interest_rate_1m = self.FCF_discounted_model_params_data['interest_rate_1m']\n",
    "        market_returns = self.returns_data['market_returns']\n",
    "\n",
    "        # 定义r_waac风险溢价\n",
    "        industry_risk_premium = r_wacc - interest_rate_1m\n",
    "        market_risk_premium_common = market_returns - interest_rate_1m\n",
    "        \n",
    "        cur = CustomRegressor(mode=self.mode, random_state=self.random_state, index=self.index)\n",
    "        if self.mode == 'train':\n",
    "            self.industry_risk_premium_common, self.risks_premiums_totals_ada_best = cur.fit_predict(market_risk_premium_common, industry_risk_premium)\n",
    "        elif self.mode == 'test':\n",
    "            self.industry_risk_premium_common, self.risks_premiums_totals_ada_best = cur.fit_predict(market_risk_premium_common, industry_risk_premium, self.risks_premiums_totals_ada_best)\n",
    "        self.industry_risk_premium_idiosyncratic = industry_risk_premium - self.industry_risk_premium_common\n",
    "\n",
    "        return self.industry_risk_premium_common, self.industry_risk_premium_idiosyncratic, self.risks_premiums_totals_ada_best\n",
    "    \n",
    "    def transform_factors_datas_from_dict_to_df(self):\n",
    "        industry_factors_dfs = pd.DataFrame(index=self.index)\n",
    "\n",
    "        for industry_factors_data in self.industry_factors_datas.values():\n",
    "            for factor_code, industry_factor_data in industry_factors_data.items():\n",
    "                industry_factors_dfs[factor_code] = industry_factor_data\n",
    "        \n",
    "        self.industry_factors_dfs = industry_factors_dfs\n",
    "        return industry_factors_dfs\n",
    "\n",
    "    \n",
    "    def get_common_and_idiosyncratic_risks_premiums_components(self): \n",
    "        risks_premiums_components = {'common_risk': {}, 'idiosyncratic_risk': {}}\n",
    "        risks_premiums_components_ada_best = {'common_risk': {}, 'idiosyncratic_risk': {}}\n",
    "        for risks_premiums_total, risks_premiums_total_name in zip(\n",
    "            [self.industry_risk_premium_common, self.industry_risk_premium_idiosyncratic],\n",
    "            ['common_risk', 'idiosyncratic_risk']\n",
    "        ):\n",
    "            for risks_premiums_component_name in ['default_risk', 'liquidity_risk','market_risk']:\n",
    "                risks_premiums_component_factors_codes = self.factors_risks_data_standardized.loc[self.factors_risks_data_standardized[risks_premiums_component_name] == 1, 'factor_code'].tolist()\n",
    "                risks_premiums_component_factors_dfs = self.industry_factors_dfs[risks_premiums_component_factors_codes]\n",
    "                \n",
    "                cur = CustomRegressor(mode=self.mode, random_state=self.random_state, index=self.index)\n",
    "                if self.mode == 'train':\n",
    "                    risks_premiums_component, risks_premiums_component_ada_best = cur.fit_predict(risks_premiums_component_factors_dfs, risks_premiums_total)\n",
    "                elif self.mode == 'test':\n",
    "                    risks_premiums_component, risks_premiums_component_ada_best = cur.fit_predict(risks_premiums_component_factors_dfs, risks_premiums_total, risks_premiums_component_ada_best)\n",
    "\n",
    "                risks_premiums_components[risks_premiums_total_name][risks_premiums_component_name] = 1\n",
    "                risks_premiums_components_ada_best[risks_premiums_total_name][risks_premiums_component_name] = 1\n",
    "\n",
    "        self.risks_premiums_components = risks_premiums_components\n",
    "        self.risks_premiums_components_ada_best = risks_premiums_components_ada_best\n",
    "        return risks_premiums_components, risks_premiums_components\n",
    "\n",
    "\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#from Modeler import Modeler\n",
    "\n",
    "mode = input('Dataset is for train or test?')\n",
    "mol = Modeler(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "handling factors datas missing values progress: 100%|██████████| 7/7 [00:00<00:00, 61.21it/s]\n"
     ]
    }
   ],
   "source": [
    "factor_data_nan_dict = mol.check_factor_data_nan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mol.factors_risks_data_standardized = pd.read_csv('factors_risks_data_standardized.csv')\n",
    "except:\n",
    "    factors_codes_missing, factors_codes_excessive, factors_codes_duplicated, mol.factors_risks_data_standardized = mol.standardize_factors_risks_data()\n",
    "    mol.factors_risks_data_standardized.to_csv('factors_risks_data_standardized.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mol.industry_factors_datas = pickle.load(open('bank_factors_datas_train.pkl', mode='rb+'))\n",
    "except:\n",
    "    mol.industry_factors_datas = mol.clean_and_average_factors_datas()\n",
    "    pickle.dump(obj=mol.industry_factors_datas, file=open(file='bank_factors_datas_train.pkl', mode='wb+'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mol.industry_r_waac_data = pickle.load(open('bank_r_waac_data_train.pkl', mode='rb+'))\n",
    "except:\n",
    "    mol.industry_r_waac_data = mol.clean_and_average_r_waac_data()\n",
    "    pickle.dump(obj=mol.industry_r_waac_data, file=open(file='bank_r_waac_data_train.pkl', mode='wb+'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mol.risks_premiums_totals_ada_best = pickle.load(open('risks_premiums_totals_ada_best.pkl', mode='rb+'))\n",
    "except:\n",
    "    _, _, mol.risks_premiums_totals_ada_best = mol.get_common_and_idiosyncratic_risks_premium()\n",
    "    pickle.dump(obj=mol.risks_premiums_totals_ada_best, file=open(file='risks_premiums_totals_ada_best.pkl', mode='wb+'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mol.factors_dfs = pd.read_csv('factors_dfs.csv')\n",
    "except:\n",
    "    mol.factors_dfs = mol.transform_factors_datas_from_dict_to_df()\n",
    "    mol.factors_dfs.to_csv('factors_dfs.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_risks_data = pd.read_csv('factors_risks_data.csv', names=['facotr_name', 'factor_code', 'risk'])\n",
    "factors_risks_data = factors_risks_data.loc[~factors_risks_data['factor_code'].isin(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_data_nan_list = factor_data_nan_dict['factors_data']\n",
    "fundamentals_data_nan_list = factor_data_nan_dict['fundamentals_data']\n",
    "macros_data_nan_list = factor_data_nan_dict['macros_data']\n",
    "\n",
    "factors_data = pickle.load(open('factors_data_train.pkl', 'rb+'))\n",
    "fundamentals_data = pickle.load(open('fundamentals_data_train.pkl', 'rb+'))\n",
    "macros_data = pickle.load(open('macros_data_train.pkl', 'rb+'))\n",
    "\n",
    "factors_data = {key: value for key, value in factors_data.items() if key not in factors_data_nan_list}\n",
    "fundamentals_data = {key: value for key, value in fundamentals_data.items() if key not in fundamentals_data_nan_list}\n",
    "macros_data = {key: value for key, value in macros_data.items() if key not in macros_data_nan_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(obj=factors_data, file=open(file=f'factors_data_train.pkl', mode='wb+'), protocol=4)\n",
    "pickle.dump(obj=fundamentals_data, file=open(file=f'fundamentals_data_train.pkl', mode='wb+'), protocol=4)\n",
    "pickle.dump(obj=macros_data, file=open(file=f'macros_data_train.pkl', mode='wb+'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#factors_risks_data = factors_risks_data.drop(columns=['index'])\n",
    "factors_risks_data = factors_risks_data.reset_index(drop=True)\n",
    "factors_risks_data.to_csv('factors_risks_data.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_codes_missing, factors_codes_excessive, factors_codes_duplicated, factors_risks_data_standardized= mol.standardize_factors_risks_data()\n",
    "factors_risks_data_standardized.to_csv('factors_risks_data_standardized.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
